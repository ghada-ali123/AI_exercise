{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3K4CYwe_JQkP"
      },
      "source": [
        "# Chapter 4: Shakespeare NLP Exercises: From Classical NLP to Neural Language Models\n",
        "\n",
        "**This exercise has 20 smaller tasks, for a total of 18 points, and you will have two weeks to complete it instead of one. Don't forget to submit your solutions to GitHub!**\n",
        "\n",
        "Welcome to this comprehensive hands-on workshop on Natural Language Processing! You will journey from classical text processing techniques to building and comparing neural language models using Shakespeare's complete works.\n",
        "\n",
        "## What You Will Learn\n",
        "\n",
        "- **Stage 0**: Corpus processing, tokenization, and word embeddings\n",
        "- **Stage 1**: Character-level RNN language models\n",
        "- **Stage 2**: Word-level RNN language models with theatrical chat interfaces\n",
        "- **Stage 3**: LSTM language models and architecture comparison\n",
        "\n",
        "## Instructions for Students\n",
        "\n",
        "Throughout this notebook, you will find code sections marked with:\n",
        "```python\n",
        "# START STUDENT CODE\n",
        "...\n",
        "# END STUDENT CODE\n",
        "```\n",
        "\n",
        "Any and all required tasks can and should be completed by writing code into these sections.\n",
        "\n",
        "XXXXX lösungen rauscutten, hinweise aus non-solution notebook wieder einpflegen\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kG1R8GtiJQkQ"
      },
      "source": [
        "## Setup and Dependencies\n",
        "\n",
        "First, let's install the required packages and check our environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrf4X7maJQkQ"
      },
      "outputs": [],
      "source": [
        "# Install required packages (uncomment if running in Colab)\n",
        "# !pip install torch numpy requests\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import requests\n",
        "from collections import Counter\n",
        "\n",
        "# Check device availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Using device: {device}\")\n",
        "if device.type == 'cuda':\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# Create data directories\n",
        "os.makedirs(\"data/works\", exist_ok=True)\n",
        "print(\"Data directories created.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "go9CEh7-JQkQ"
      },
      "source": [
        "---\n",
        "\n",
        "# Stage 0: Corpus & Classical NLP Foundations\n",
        "\n",
        "In this stage, you will work with the complete works of Shakespeare to learn fundamental text processing techniques:\n",
        "- Downloading and segmenting a large text corpus\n",
        "- Tokenization and normalization\n",
        "- Working with pretrained word embeddings (GloVe)\n",
        "\n",
        "---\n",
        "\n",
        "## Stage 0.1 – Shakespeare Corpus Download & Segmentation\n",
        "\n",
        "This exercise introduces you to working with a **real, unstructured text corpus**. You will download the complete works of William Shakespeare from Project Gutenberg and convert the raw file into a collection of **separate, clean text files**, one per work. To make sure everyone can actually start working on the exercises and gets stuck right at the start, we provide code that starts you off and does the following things for you:\n",
        "\n",
        "1. **Download the Shakespeare corpus** from Project Gutenberg\n",
        "2. **Analyze the structure** - find the Table of Contents and title markers\n",
        "3. **Segment the corpus** into separate work files\n",
        "4. **Verify your segmentation** by printing statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNVIR58lJQkR"
      },
      "outputs": [],
      "source": [
        "RAW_FILE = 'data/pg100.txt'\n",
        "WORKS_DIR = 'data/works'\n",
        "\n",
        "def download_corpus():\n",
        "    \"\"\"Download the Shakespeare corpus from Project Gutenberg if not present.\"\"\"\n",
        "\n",
        "    if not os.path.exists(RAW_FILE):\n",
        "        print(f\"Downloading corpus...\")\n",
        "        url = \"https://www.gutenberg.org/cache/epub/100/pg100.txt\"\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()\n",
        "            with open(RAW_FILE, 'wb') as f:\n",
        "                f.write(response.content)\n",
        "            print(\"Download complete.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error downloading file: {e}\")\n",
        "            return False\n",
        "    else:\n",
        "        print(f\"Found {RAW_FILE}, skipping download.\")\n",
        "    return True\n",
        "\n",
        "\n",
        "def segment_corpus():\n",
        "    \"\"\"Segment the corpus into individual works.\"\"\"\n",
        "    # Read the file\n",
        "    with open(RAW_FILE, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    # Find Table of Contents\n",
        "    toc_start_idx = -1\n",
        "    for i, line in enumerate(lines):\n",
        "        if \"Contents\" in line and len(line.strip()) < 20:\n",
        "            if i < 200:  # TOC should be in first 200 lines\n",
        "                toc_start_idx = i\n",
        "                break\n",
        "\n",
        "    if toc_start_idx == -1:\n",
        "        print(\"Could not find Table of Contents.\")\n",
        "        return []\n",
        "\n",
        "    print(f\"Found Table of Contents around line {toc_start_idx + 1}\")\n",
        "\n",
        "    # Extract titles from TOC\n",
        "    potential_titles = []\n",
        "    first_candidate = None\n",
        "    toc_end_idx = -1\n",
        "\n",
        "    for i in range(toc_start_idx + 1, len(lines)):\n",
        "        stripped = lines[i].strip()\n",
        "        if not stripped:\n",
        "            continue\n",
        "\n",
        "        if first_candidate is None:\n",
        "            first_candidate = stripped\n",
        "            potential_titles.append(stripped)\n",
        "            continue\n",
        "\n",
        "        # Check if this line matches the first candidate (start of first work)\n",
        "        if stripped == first_candidate:\n",
        "            toc_end_idx = i\n",
        "            break\n",
        "\n",
        "        potential_titles.append(stripped)\n",
        "\n",
        "        if i > 3000:  # Safety break\n",
        "            print(\"Warning: TOC parsing went too far.\")\n",
        "            break\n",
        "\n",
        "    if toc_end_idx == -1:\n",
        "        print(\"Could not determine end of TOC.\")\n",
        "        return []\n",
        "\n",
        "    works_titles = potential_titles\n",
        "    print(f\"Identified {len(works_titles)} works from TOC.\")\n",
        "\n",
        "    # Find start positions of each work\n",
        "    work_starts = {}\n",
        "    current_search_idx = toc_end_idx\n",
        "    work_starts[works_titles[0]] = current_search_idx\n",
        "\n",
        "    for k in range(1, len(works_titles)):\n",
        "        title = works_titles[k]\n",
        "        found = False\n",
        "        for j in range(current_search_idx + 1, len(lines)):\n",
        "            if lines[j].strip() == title:\n",
        "                work_starts[title] = j\n",
        "                current_search_idx = j\n",
        "                found = True\n",
        "                break\n",
        "        if not found:\n",
        "            print(f\"Warning: Could not find start of '{title}'\")\n",
        "\n",
        "    # Write to files\n",
        "    os.makedirs(WORKS_DIR, exist_ok=True)\n",
        "    sorted_works = sorted(work_starts.items(), key=lambda x: x[1])\n",
        "\n",
        "    extracted_works = []\n",
        "    for i in range(len(sorted_works)):\n",
        "        title, start_line = sorted_works[i]\n",
        "\n",
        "        if i < len(sorted_works) - 1:\n",
        "            end_line = sorted_works[i + 1][1]\n",
        "        else:\n",
        "            end_line = len(lines)\n",
        "\n",
        "        content_lines = lines[start_line:end_line]\n",
        "        text_content = \"\".join(content_lines)\n",
        "\n",
        "        # Clean title for filename\n",
        "        safe_title = re.sub(r'[^\\w\\s-]', '', title).strip().lower()\n",
        "        safe_title = re.sub(r'[\\s-]+', '_', safe_title)\n",
        "        filename = f\"{safe_title}.txt\"\n",
        "\n",
        "        out_path = os.path.join(WORKS_DIR, filename)\n",
        "        with open(out_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(text_content)\n",
        "\n",
        "        extracted_works.append((title, filename, len(text_content), len(content_lines)))\n",
        "\n",
        "    return extracted_works\n",
        "    # Task 0.2: END STUDENT CODE\n",
        "\n",
        "# Run the corpus download and segmentation\n",
        "if download_corpus():\n",
        "    works = segment_corpus()\n",
        "\n",
        "    if works:\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"SEGMENTATION SUMMARY\")\n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\"{'Work Title':<50} {'Chars':>10} {'Lines':>8}\")\n",
        "        print(\"-\" * 70)\n",
        "        for title, filename, chars, lines_count in works:\n",
        "            print(f\"{title[:48]:<50} {chars:>10} {lines_count:>8}\")\n",
        "        print(f\"\\nTotal works extracted: {len(works)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlC1EEi9JQkR"
      },
      "source": [
        "---\n",
        "\n",
        "## Exercise 0.2 – Basic Tokenization and Normalization\n",
        "\n",
        "### Description\n",
        "\n",
        "In this exercise, you will build foundational text-processing utilities. You will design a simple **tokenizer** and apply basic **normalization** steps to Shakespeare's works. This mirrors the early stages of many NLP pipelines.\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "After completing this exercise, you should be able to:\n",
        "- Implement a minimal **tokenizer** for plain-text data\n",
        "- Apply common **normalization** steps such as lowercasing and punctuation handling\n",
        "- Inspect token distributions to understand corpus characteristics\n",
        "- Compute statistics for single works and the entire corpus\n",
        "\n",
        "### Tasks\n",
        "\n",
        "1. **Implement a basic tokenizer (3x0.5 points)** that:\n",
        "   - Splits text into word-like units\n",
        "   - Treats whitespace as a separator\n",
        "   - Separates punctuation into its own tokens\n",
        "   \n",
        "2. **Add normalization steps (1 point)**:\n",
        "   - Lowercase all tokens\n",
        "   - Normalize curly/smart quotes to straight quotes\n",
        "   \n",
        "3. **Inspect tokenized output** and compute statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHbteSPuJQkR"
      },
      "outputs": [],
      "source": [
        "# Exercise 0.2: Basic Tokenization and Normalization\n",
        "\n",
        "\n",
        "def tokenize(text: str) -> list[str]:\n",
        "    \"\"\"\n",
        "    Tokenizes the input text into a list of strings.\n",
        "\n",
        "    Design Decisions:\n",
        "    1. Lowercase: Applied to reduce vocabulary size.\n",
        "    2. Punctuation: Separated from words into their own tokens.\n",
        "    3. Contractions: Kept together using regex (e.g., \"don't\" stays as one token).\n",
        "\n",
        "    Args:\n",
        "        text: Input text string\n",
        "\n",
        "    Returns:\n",
        "        List of token strings\n",
        "    \"\"\"\n",
        "    # Task 0.3: START STUDENT CODE\n",
        "\n",
        "    # 1. Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # 2. Normalize smart quotes to straight quotes\n",
        "    text = text.replace(\"’\", \"'\").replace(\"‘\", \"'\")\n",
        "    text = text.replace(\"“\", '\"').replace(\"”\", '\"')\n",
        "\n",
        "    # 3. Normalize whitespace: replace newlines/tabs with space\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "\n",
        "    # 4. Extract tokens:\n",
        "    #    - Words with optional contractions: \\w+(?:'\\w+)?\n",
        "    #    - Single punctuation characters: [^\\w\\s]\n",
        "    pattern = r\"\\w+(?:'\\w+)?|[^\\w\\s]\"\n",
        "    tokens = re.findall(pattern, text)\n",
        "\n",
        "    return tokens\n",
        "    # HINT:\n",
        "    # 1. Lowercase the text\n",
        "    # 2. Normalize smart quotes ('', \"\") to straight quotes (' ', \"\")\n",
        "    # 3. Replace multiple whitespace chars (newlines, tabs) with single space\n",
        "    # 4. Use re.findall() with a regex pattern to extract tokens:\n",
        "    #    - Words with optional contractions: \\w+(?:'\\w+)?\n",
        "    #    - Single punctuation: [^\\w\\s]\n",
        "    # 5. Return the list of tokens\n",
        "    pass\n",
        "\n",
        "    # Task 0.3: END STUDENT CODE\n",
        "\n",
        "# Test the tokenizer on a sample work\n",
        "sample_work = 'the_tragedy_of_romeo_and_juliet.txt'\n",
        "sample_path = os.path.join(WORKS_DIR, sample_work)\n",
        "\n",
        "if os.path.exists(sample_path):\n",
        "    with open(sample_path, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "\n",
        "    tokens = tokenize(text)\n",
        "    vocab = Counter(tokens)\n",
        "\n",
        "    print(f\"Analyzing: {sample_work}\")\n",
        "    print(f\"\\nTotal tokens: {len(tokens)}\")\n",
        "    print(f\"Unique tokens (vocabulary size): {len(vocab)}\")\n",
        "\n",
        "    print(\"\\n--- First 50 tokens ---\")\n",
        "    print(tokens[:50])\n",
        "\n",
        "    print(\"\\n--- Top 10 most frequent tokens ---\")\n",
        "    for token, count in vocab.most_common(10):\n",
        "        print(f\"  '{token}': {count}\")\n",
        "\n",
        "    print(\"\\n--- Sample rare tokens ---\")\n",
        "    for token, count in vocab.most_common()[-5:]:\n",
        "        print(f\"  '{token}': {count}\")\n",
        "else:\n",
        "    print(f\"Sample work not found at {sample_path}. Run Exercise 0.1 first.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPdxk4hxJQkR"
      },
      "outputs": [],
      "source": [
        "# Exercise 0.2 (continued): Corpus-wide Statistics\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"CORPUS-WIDE STATISTICS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Collect statistics from all works\n",
        "work_files = [f for f in os.listdir(WORKS_DIR) if f.endswith('.txt')]\n",
        "print(f\"\\nFound {len(work_files)} works in the corpus.\\n\")\n",
        "\n",
        "corpus_tokens = []\n",
        "corpus_vocab = Counter()\n",
        "work_stats = []\n",
        "\n",
        "for work_file in sorted(work_files):\n",
        "    work_path = os.path.join(WORKS_DIR, work_file)\n",
        "    with open(work_path, 'r', encoding='utf-8') as f:\n",
        "        work_text = f.read()\n",
        "\n",
        "    work_tokens = tokenize(work_text)\n",
        "    work_vocab = Counter(work_tokens)\n",
        "\n",
        "    corpus_tokens.extend(work_tokens)\n",
        "    corpus_vocab.update(work_vocab)\n",
        "\n",
        "    work_stats.append({\n",
        "        'name': work_file,\n",
        "        'total_tokens': len(work_tokens),\n",
        "        'unique_tokens': len(work_vocab)\n",
        "    })\n",
        "\n",
        "# Print per-work summary\n",
        "print(f\"{'Work':<50} {'Total Tokens':>12} {'Unique Tokens':>14}\")\n",
        "print(\"-\" * 76)\n",
        "for stat in sorted(work_stats, key=lambda x: x['total_tokens'], reverse=True)[:10]:\n",
        "    print(f\"{stat['name']:<50} {stat['total_tokens']:>12} {stat['unique_tokens']:>14}\")\n",
        "print(\"... (showing top 10 by token count)\")\n",
        "\n",
        "# Print corpus-wide statistics\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"AGGREGATED STATISTICS\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nTotal tokens across all works: {len(corpus_tokens):,}\")\n",
        "print(f\"Unique tokens (vocabulary size): {len(corpus_vocab):,}\")\n",
        "print(f\"Average tokens per work: {len(corpus_tokens) / len(work_files):,.0f}\")\n",
        "\n",
        "print(\"\\n--- Top 20 Most Common Tokens in Corpus ---\")\n",
        "for token, count in corpus_vocab.most_common(20):\n",
        "    print(f\"  {token:>15}: {count:>8} occurrences\")\n",
        "\n",
        "print(\"\\n--- Token Coverage Statistics ---\")\n",
        "total_tokens = len(corpus_tokens)\n",
        "for n in [10, 50, 100, 500, 1000]:\n",
        "    top_n_count = sum(count for _, count in corpus_vocab.most_common(n))\n",
        "    coverage = (top_n_count / total_tokens) * 100\n",
        "    print(f\"  Top {n:>4} tokens cover {coverage:>5.1f}% of all tokens\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_m3DOuAtJQkR"
      },
      "source": [
        "---\n",
        "\n",
        "## Exercise 0.3 – Working With Pretrained Word Embeddings\n",
        "\n",
        "### Description\n",
        "\n",
        "In this exercise, you will download a widely used pretrained word embedding model (GloVe) and explore semantic relationships between words by computing cosine similarities manually.\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "After completing this exercise, you should be able to:\n",
        "- Load a pretrained embedding model into Python\n",
        "- Implement cosine similarity manually using tensor operations\n",
        "- Compute and analyze similarities between selected word pairs\n",
        "- Observe how semantic and syntactic relationships are reflected in vector space\n",
        "- Perform analogy operations (e.g., king - man + woman ≈ queen)\n",
        "\n",
        "### Tasks\n",
        "\n",
        "1. Download GloVe embeddings (100-dimensional version)\n",
        "2. Load embeddings into a dictionary\n",
        "3. **Implement cosine similarity** manually **(0.5 points)**\n",
        "4. Explore semantic relationships** between word pairs - this requires **building a find_nearest function** that finds the nearest neighbor from a set of input vectors and a target vector. **(1 point)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lx3XhJnhJQkR"
      },
      "outputs": [],
      "source": [
        "# Exercise 0.3: Working With Pretrained Word Embeddings\n",
        "\n",
        "GLOVE_PATH = 'data/glove.6B.100d.txt'\n",
        "\n",
        "\n",
        "def load_glove(path: str) -> dict:\n",
        "    \"\"\"\n",
        "    Load GloVe embeddings from a file.\n",
        "\n",
        "    Args:\n",
        "        path: Path to the GloVe file\n",
        "\n",
        "    Returns:\n",
        "        Dictionary mapping words to torch tensors\n",
        "    \"\"\"\n",
        "    print(f\"Loading GloVe from {path}...\")\n",
        "    embeddings = {}\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            try:\n",
        "                vector = np.array(values[1:], dtype='float32')\n",
        "                if len(vector) == 100:  # Ensure correct dimension\n",
        "                    embeddings[word] = torch.tensor(vector)\n",
        "            except ValueError:\n",
        "                continue\n",
        "    print(f\"Loaded {len(embeddings)} words.\")\n",
        "    return embeddings\n",
        "\n",
        "\n",
        "def cosine_similarity(vec1: torch.Tensor, vec2: torch.Tensor) -> float:\n",
        "    \"\"\"\n",
        "    Compute cosine similarity between two vectors manually.\n",
        "\n",
        "    Formula: cos(θ) = (A · B) / (|A| * |B|)\n",
        "\n",
        "    Args:\n",
        "        vec1: First vector\n",
        "        vec2: Second vector\n",
        "\n",
        "    Returns:\n",
        "        Cosine similarity as a float\n",
        "    \"\"\"\n",
        "    # Task 1.1: START STUDENT CODE\n",
        "\n",
        "    # HINT: Implement the cosine similarity formula: cos(θ) = (A · B) / (|A| * |B|)\n",
        "    # 1. Compute dot product: torch.dot(vec1, vec2)\n",
        "    # 2. Compute norms: torch.norm()\n",
        "    # 3. Handle zero norms (avoid division by zero)\n",
        "    # 4. Return the result as a Python float using .item()\n",
        "    pass\n",
        "\n",
        "    # Task 1.1: END STUDENT CODE\n",
        "\n",
        "\n",
        "def find_nearest(embeddings: dict, target_vec: torch.Tensor, n: int = 5,\n",
        "                 exclude_words: list = None) -> list:\n",
        "    \"\"\"\n",
        "    Find the n nearest neighbors to a target vector by computing\n",
        "    the dot product of each word embedding with the target vector.\n",
        "\n",
        "    Args:\n",
        "        embeddings: Dictionary of word embeddings\n",
        "        target_vec: Target vector to find neighbors for\n",
        "        n: Number of neighbors to return\n",
        "        exclude_words: Words to exclude from results\n",
        "\n",
        "    Returns:\n",
        "        List of (word, similarity) tuples\n",
        "    \"\"\"\n",
        "    # Task 1.2: START STUDENT CODE\n",
        "\n",
        "    # HINT:\n",
        "    # 1. Convert embeddings dict to lists: words and vocab_matrix (stacked tensors)\n",
        "    # 2. Compute norms for all vocabulary vectors and the target vector\n",
        "    # 3. Compute dot products between vocab_matrix and target_vec using matmul\n",
        "    # 4. Compute cosine similarities using the formula\n",
        "    # 5. Use torch.topk() to find top k highest scores\n",
        "    # 6. Filter out excluded_words and return top n results as (word, score) tuples\n",
        "    pass\n",
        "\n",
        "    # Task 1.2: END STUDENT CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJEIH3PNYwVJ"
      },
      "outputs": [],
      "source": [
        "# Download GLOVE - you only need to do this once.\n",
        "\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip -d data/\n",
        "\n",
        "# If this fails, you can download manually from:\n",
        "# https://nlp.stanford.edu/data/glove.6B.zip\n",
        "# and extract the zip file into the /data directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8JFTFzVYwVK"
      },
      "outputs": [],
      "source": [
        "# Load embeddings and explore relationships\n",
        "\n",
        "if os.path.exists(GLOVE_PATH):\n",
        "    embeddings = load_glove(GLOVE_PATH)\n",
        "\n",
        "    # Explore semantic relationships\n",
        "    pairs = [\n",
        "        (\"king\", \"monarch\"), (\"love\", \"affection\"),  # Synonyms\n",
        "        (\"war\", \"peace\"), (\"love\", \"hate\"),  # Antonyms\n",
        "        (\"doctor\", \"nurse\"), (\"poison\", \"dagger\"),  # Semantic fields\n",
        "        (\"romeo\", \"juliet\"), (\"tragedy\", \"comedy\")  # Shakespeare-related\n",
        "    ]\n",
        "\n",
        "    print(\"\\n--- Cosine Similarity Between Word Pairs ---\")\n",
        "    for w1, w2 in pairs:\n",
        "        if w1 in embeddings and w2 in embeddings:\n",
        "            sim = cosine_similarity(embeddings[w1], embeddings[w2])\n",
        "            print(f\"  {w1:12} - {w2:12}: {sim:.4f}\")\n",
        "        else:\n",
        "            missing = [w for w in [w1, w2] if w not in embeddings]\n",
        "            print(f\"  Missing: {missing}\")\n",
        "\n",
        "    # Analogies\n",
        "    print(\"\\n--- Word Analogies ---\")\n",
        "\n",
        "    def solve_analogy(pos1, neg1, pos2):\n",
        "        \"\"\"Solve: pos1 - neg1 + pos2 = ?\"\"\"\n",
        "        print(f\"\\n  {pos1} - {neg1} + {pos2} = ?\")\n",
        "        if all(w in embeddings for w in [pos1, neg1, pos2]):\n",
        "            vec = embeddings[pos1] - embeddings[neg1] + embeddings[pos2]\n",
        "            neighbors = find_nearest(embeddings, vec, n=5, exclude_words=[pos1, neg1, pos2])\n",
        "            for word, score in neighbors:\n",
        "                print(f\"    {word}: {score:.4f}\")\n",
        "        else:\n",
        "            print(\"    Word(s) not in vocabulary.\")\n",
        "\n",
        "    solve_analogy(\"king\", \"man\", \"woman\")  # Expected: queen\n",
        "    solve_analogy(\"paris\", \"france\", \"italy\")  # Expected: rome\n",
        "    solve_analogy(\"father\", \"man\", \"woman\")  # Expected: mother\n",
        "else:\n",
        "    print(f\"GloVe file not found at {GLOVE_PATH}\")\n",
        "    print(\"Please download from: https://nlp.stanford.edu/data/glove.6B.zip\")\n",
        "    print(\"Extract glove.6B.100d.txt to the data/ directory.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-kj6IzRJQkR"
      },
      "source": [
        "---\n",
        "\n",
        "# Stage 1: Character-Level Language Modeling\n",
        "\n",
        "In this stage, you will build your first **neural language model** over Shakespeare's text using a **character-level recurrent neural network (RNN)** in PyTorch. You will:\n",
        "\n",
        "- Construct a character-level dataset from one work\n",
        "- Implement and train an RNN-based language model (on GPU if available)\n",
        "- Generate text using greedy decoding and temperature sampling\n",
        "\n",
        "---\n",
        "\n",
        "## Exercise 1.1 – Character Vocabulary and Sequential Dataset\n",
        "\n",
        "### Description\n",
        "\n",
        "You will construct a **character-level representation** of a Shakespeare work and prepare a dataset for **next-character prediction**.\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "After completing this exercise, you should be able to:\n",
        "- Build a **character vocabulary** from raw text\n",
        "- Map characters to integer indices and back\n",
        "- Prepare sliding-window input–target pairs for sequence modeling\n",
        "- Wrap the data in a PyTorch `Dataset` and `DataLoader`\n",
        "\n",
        "### Task\n",
        "\n",
        "Implement a PyTorch Dataset class that implements the usual functions; \\_\\_init\\_\\_, \\_\\_len\\_\\_, and \\_\\_getitem\\_\\_. The init function needs to take the text corpus as a string and a sequence length as an integer. The getitem function takes an index integer as usual and returns a number of characters (sequence length) starting at that index in the text corpus - this is the data - and a target that has the same length, but which is offset by one to the right. For example, if the corpus is the text `Hello, I am a dog.`, then the output at sequence length 5 and index 0 would be `Hello` (data) and `ello,` (target). **(3 x 0.5 points)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CGVClVmJQkS"
      },
      "outputs": [],
      "source": [
        "# Exercise 1.1: Character Vocabulary and Sequential Dataset\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "\n",
        "class CharDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A PyTorch Dataset for character-level language modeling.\n",
        "\n",
        "    Creates input-target pairs using a sliding window over the text.\n",
        "    Input: characters from position t to t + seq_len - 1\n",
        "    Target: characters from position t + 1 to t + seq_len\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, text: str, seq_len: int):\n",
        "        \"\"\"\n",
        "        Initialize the dataset.\n",
        "\n",
        "        Args:\n",
        "            text: The full text as a string\n",
        "            seq_len: Length of each sequence (number of characters)\n",
        "        \"\"\"\n",
        "        # Task 1.3: START STUDENT CODE\n",
        "\n",
        "        # HINT:\n",
        "        # 1. Store text and seq_len\n",
        "        # 2. Build character set: unique chars from text, sorted for consistency\n",
        "        # 3. Create bidirectional mappings: char_to_idx and idx_to_char\n",
        "        # 4. Encode entire text as tensor of indices using char_to_idx\n",
        "        pass\n",
        "\n",
        "        # Task 1.3: END STUDENT CODE\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of samples we can create from the text.\"\"\"\n",
        "        # Task 1.4: START STUDENT CODE\n",
        "\n",
        "        # HINT: Each sample needs seq_len + 1 characters (seq_len for input, last one for target)\n",
        "        # So max samples = len(self.data) - seq_len\n",
        "        pass\n",
        "\n",
        "        # Task 1.4: END STUDENT CODE\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Get a single sample.\n",
        "\n",
        "        Returns:\n",
        "            input_seq: tensor of shape [seq_len]\n",
        "            target_seq: tensor of shape [seq_len]\n",
        "        \"\"\"\n",
        "        # Task 1.5: START STUDENT CODE\n",
        "\n",
        "        # HINT: Implement sliding window for next-character prediction\n",
        "        # 1. Extract seq_len + 1 characters starting at idx\n",
        "        # 2. Split into input (first seq_len) and target (last seq_len, shifted by 1)\n",
        "        # 3. Return both as tensors\n",
        "        pass\n",
        "\n",
        "        # Task 1.5: END STUDENT CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HE_gHAxMYwVL"
      },
      "outputs": [],
      "source": [
        "# Test the CharDataset\n",
        "print(\"--- Exercise 1.1: Character Dataset ---\")\n",
        "\n",
        "# Load Romeo and Juliet (or another work)\n",
        "work_path = os.path.join(WORKS_DIR, 'the_tragedy_of_romeo_and_juliet.txt')\n",
        "if not os.path.exists(work_path):\n",
        "    print(\"Work file not found. Please run Exercise 0.1 first.\")\n",
        "else:\n",
        "    with open(work_path, 'r', encoding='utf-8') as f:\n",
        "        char_text = f.read()\n",
        "\n",
        "    print(f\"Loaded text length: {len(char_text)} characters\")\n",
        "\n",
        "    # Create dataset\n",
        "    # seq_len=100 captures roughly 1-2 lines of verse - good context for learning structure\n",
        "    CHAR_SEQ_LEN = 100\n",
        "    CHAR_BATCH_SIZE = 64\n",
        "\n",
        "    char_dataset = CharDataset(char_text, CHAR_SEQ_LEN)\n",
        "    print(f\"Vocabulary Size: {char_dataset.vocab_size}\")\n",
        "    print(f\"Characters: {repr(''.join(char_dataset.char_to_idx.keys()))}\")\n",
        "    print(f\"Number of samples: {len(char_dataset)}\")\n",
        "\n",
        "    # Create DataLoader\n",
        "    char_dataloader = DataLoader(char_dataset, batch_size=CHAR_BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    # Verify with one batch\n",
        "    inputs, targets = next(iter(char_dataloader))\n",
        "    print(f\"\\nBatch Input Shape: {inputs.shape}\")\n",
        "    print(f\"Batch Target Shape: {targets.shape}\")\n",
        "\n",
        "    # Show a sample\n",
        "    sample_input = \"\".join([char_dataset.idx_to_char[i.item()] for i in inputs[0]])\n",
        "    sample_target = \"\".join([char_dataset.idx_to_char[i.item()] for i in targets[0]])\n",
        "    print(f\"\\nSample Input (first 50 chars): {repr(sample_input[:50])}\")\n",
        "    print(f\"Sample Target (first 50 chars): {repr(sample_target[:50])}\")\n",
        "\n",
        "    print(f\"\\n--- Design Notes ---\")\n",
        "    print(f\"seq_len={CHAR_SEQ_LEN}: Captures sufficient context (approx 1-2 lines of verse)\")\n",
        "    print(f\"batch_size={CHAR_BATCH_SIZE}: Good balance between efficiency and memory usage\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2soyETvJQkS"
      },
      "source": [
        "---\n",
        "\n",
        "## Exercise 1.2 – Character-Level RNN Language Model in PyTorch\n",
        "\n",
        "### Description\n",
        "\n",
        "You will implement and train a **character-level RNN-based language model** using the dataset from Exercise 1.1. The model will learn to predict the next character given the previous characters.\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "After completing this exercise, you should be able to:\n",
        "- Define a simple **recurrent neural network** for language modeling\n",
        "- Use an **embedding layer**, an `nn.RNN`, and a linear output layer\n",
        "- Train a neural language model with **cross-entropy loss**\n",
        "- Run training on **GPU** where available\n",
        "\n",
        "### Model Architecture\n",
        "\n",
        "```\n",
        "Input (char indices) → Embedding → RNN → Linear → Output (vocab logits)\n",
        "```\n",
        "\n",
        "### Tasks\n",
        "\n",
        "1. Set up device selection (GPU if available)\n",
        "2. Implement the `CharRNNLM` model class **(1 point)**\n",
        "3. Train the model with cross-entropy loss and Adam optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdhl6pY8JQkS"
      },
      "outputs": [],
      "source": [
        "# Exercise 1.2: Character-Level RNN Language Model\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class CharRNNLM(nn.Module):\n",
        "    \"\"\"\n",
        "    Character-level RNN Language Model.\n",
        "\n",
        "    Architecture:\n",
        "    - Embedding layer: maps character indices to dense vectors\n",
        "    - RNN layer: processes sequences and maintains hidden state\n",
        "    - Linear layer: maps hidden states to vocabulary logits\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size: int, emb_dim: int, hidden_size: int):\n",
        "        \"\"\"\n",
        "        Initialize the model.\n",
        "\n",
        "        Args:\n",
        "            vocab_size: Number of unique characters\n",
        "            emb_dim: Dimension of character embeddings\n",
        "            hidden_size: Number of hidden units in RNN\n",
        "        \"\"\"\n",
        "        # Task 1.6: START STUDENT CODE\n",
        "\n",
        "        # HINT: Build a simple RNN-based language model\n",
        "        # 1. Call super().__init__()\n",
        "        # 2. Create embedding layer: nn.Embedding(vocab_size, emb_dim)\n",
        "        # 3. Create RNN layer: nn.RNN(emb_dim, hidden_size, batch_first=True)\n",
        "        # 4. Create linear output layer: nn.Linear(hidden_size, vocab_size)\n",
        "        pass\n",
        "\n",
        "        # Task 1.6: END STUDENT CODE\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch, seq_len)\n",
        "            hidden: Optional initial hidden state\n",
        "\n",
        "        Returns:\n",
        "            logits: Output logits of shape (batch, seq_len, vocab_size)\n",
        "            hidden: Final hidden state\n",
        "        \"\"\"\n",
        "        # Task 1.7: START STUDENT CODE\n",
        "\n",
        "        # HINT: Forward pass through the model:\n",
        "        # 1. Embed the input indices\n",
        "        # 2. Pass through RNN to get hidden states\n",
        "        # 3. Pass hidden states through linear layer to get logits\n",
        "        # 4. Return logits and final hidden state\n",
        "        pass\n",
        "\n",
        "        # Task 1.7: END STUDENT CODE\n",
        "\n",
        "# Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T903E9MSJQkS"
      },
      "outputs": [],
      "source": [
        "# Exercise 1.2 (continued): Training the Character RNN\n",
        "\n",
        "print(\"--- Exercise 1.2: Training Character RNN ---\")\n",
        "\n",
        "# Model hyperparameters\n",
        "CHAR_EMB_DIM = 64\n",
        "CHAR_HIDDEN_SIZE = 256\n",
        "CHAR_EPOCHS = 5\n",
        "CHAR_LR = 0.002\n",
        "\n",
        "# Create model\n",
        "char_model = CharRNNLM(\n",
        "    vocab_size=char_dataset.vocab_size,\n",
        "    emb_dim=CHAR_EMB_DIM,\n",
        "    hidden_size=CHAR_HIDDEN_SIZE\n",
        ").to(device)\n",
        "\n",
        "print(f\"Model created with {sum(p.numel() for p in char_model.parameters()):,} parameters\")\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(char_model.parameters(), lr=CHAR_LR)\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n--- Training Notes ---\")\n",
        "print(\"The loss should decrease over epochs, indicating the model is learning.\")\n",
        "print(\"If loss doesn't decrease, try: lower learning rate, more epochs, or larger model.\")\n",
        "\n",
        "# Training loop\n",
        "char_model.train()\n",
        "for epoch in range(CHAR_EPOCHS):\n",
        "    total_loss = 0\n",
        "    for i, (inputs, targets) in enumerate(char_dataloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass (hidden state starts as None/zeros)\n",
        "        logits, _ = char_model(inputs)\n",
        "\n",
        "        # Reshape for loss: (batch * seq_len, vocab_size) vs (batch * seq_len)\n",
        "        loss = criterion(logits.view(-1, char_dataset.vocab_size), targets.view(-1))\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(char_model.parameters(), 5)  # Gradient clipping for stability\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(char_dataloader)\n",
        "    print(f\"Epoch {epoch+1}/{CHAR_EPOCHS}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Save model checkpoint\n",
        "char_model_path = \"char_rnn_model.pth\"\n",
        "torch.save({\n",
        "    'model_state_dict': char_model.state_dict(),\n",
        "    'vocab_size': char_dataset.vocab_size,\n",
        "    'emb_dim': CHAR_EMB_DIM,\n",
        "    'hidden_size': CHAR_HIDDEN_SIZE,\n",
        "    'char_to_idx': char_dataset.char_to_idx,\n",
        "    'idx_to_char': char_dataset.idx_to_char,\n",
        "}, char_model_path)\n",
        "print(f\"\\nModel saved to {char_model_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAhqSmaVJQkS"
      },
      "source": [
        "---\n",
        "\n",
        "## Exercise 1.3 – Text Generation and Temperature Sampling\n",
        "\n",
        "### Description\n",
        "\n",
        "You will implement text generation functions for your character-level language model and experiment with different **sampling strategies**, including **greedy decoding** and **temperature-scaled sampling**.\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "After completing this exercise, you should be able to:\n",
        "- Generate text autoregressively from a trained character-level language model\n",
        "- Implement **greedy decoding** and observe its limitations\n",
        "- Implement **temperature-based sampling** from a categorical distribution\n",
        "- Qualitatively compare generated outputs at different temperatures\n",
        "\n",
        "### Temperature Explained\n",
        "\n",
        "- **Temperature = 0 (or very low)**: Greedy - always picks most likely character. Repetitive but \"safe\".\n",
        "- **Temperature = 1.0**: Standard sampling from the learned distribution.\n",
        "- **Temperature > 1.0**: More random/creative, but may produce nonsense.\n",
        "- **Temperature < 1.0**: More focused/conservative, less variety.\n",
        "\n",
        "### Tasks\n",
        "\n",
        "1. Implement a `generate_greedy` function - This function should generate a string of predefined length, given a model and a start_text string. **(1 point)**\n",
        "2. Implement a `generate_with_temperature` function - This function should generate a string of predefined length, given a model and a start_text string, but the generation of each new character is up to chance. The probabilities that a possible new character is selected is based on the temperature-scaled logits (our normal outputs times the temperature factor). **(1 point)**\n",
        "3. Compare outputs at different temperatures. **(0.5 points)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zTu0R_jJQkS"
      },
      "outputs": [],
      "source": [
        "# Exercise 1.3: Text Generation and Temperature Sampling\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def generate_greedy(model, start_text: str, length: int, char_to_idx: dict,\n",
        "                    idx_to_char: dict, device='cpu') -> str:\n",
        "    \"\"\"\n",
        "    Generate text using greedy decoding (always pick most likely next character).\n",
        "\n",
        "    Args:\n",
        "        model: Trained CharRNNLM model\n",
        "        start_text: Initial prompt text\n",
        "        length: Number of characters to generate\n",
        "        char_to_idx: Character to index mapping\n",
        "        idx_to_char: Index to character mapping\n",
        "        device: Device to run on\n",
        "\n",
        "    Returns:\n",
        "        Generated text string (including prompt)\n",
        "    \"\"\"\n",
        "    # Task 1.8: START STUDENT CODE\n",
        "\n",
        "    # HINT: Implement greedy decoding for text generation\n",
        "    # 1. Set model to eval mode and move to device\n",
        "    # 2. Encode the start_text as character indices (handle unknown with fallback)\n",
        "    # 3. Process prompt through model to initialize hidden state\n",
        "    # 4. Loop for 'length' iterations:\n",
        "    #    - Pick character with highest probability (torch.argmax)\n",
        "    #    - Append to generated_text\n",
        "    #    - Feed single character to model for next step\n",
        "    # 5. Return generated text\n",
        "    pass\n",
        "\n",
        "    # Task 1.8: END STUDENT CODE\n",
        "\n",
        "\n",
        "def generate_with_temperature(model, start_text: str, length: int, temperature: float,\n",
        "                              char_to_idx: dict, idx_to_char: dict, device='cpu') -> str:\n",
        "    \"\"\"\n",
        "    Generate text using temperature-scaled sampling.\n",
        "\n",
        "    Args:\n",
        "        model: Trained CharRNNLM model\n",
        "        start_text: Initial prompt text\n",
        "        length: Number of characters to generate\n",
        "        temperature: Sampling temperature (higher = more random)\n",
        "        char_to_idx: Character to index mapping\n",
        "        idx_to_char: Index to character mapping\n",
        "        device: Device to run on\n",
        "\n",
        "    Returns:\n",
        "        Generated text string (including prompt)\n",
        "    \"\"\"\n",
        "    # Task 1.9: START STUDENT CODE\n",
        "\n",
        "    # HINT: Implement temperature-scaled sampling for more creative text\n",
        "    # 1. Set model to eval mode and move to device\n",
        "    # 2. Encode start_text and process through model\n",
        "    # 3. Loop for 'length' iterations:\n",
        "    #    - If temperature very small (~0): use greedy (torch.argmax)\n",
        "    #    - Otherwise: scale logits by temperature, softmax to probabilities, sample\n",
        "    #    - Use torch.multinomial() to sample from probability distribution\n",
        "    #    - Append character and feed to model\n",
        "    # 4. Return generated text\n",
        "    pass\n",
        "\n",
        "    # Task 1.9: END STUDENT CODE\n",
        "\n",
        "# Test generation\n",
        "print(\"--- Exercise 1.3: Text Generation ---\")\n",
        "\n",
        "prompts = [\"ROMEO.\", \"The \"]\n",
        "\n",
        "for prompt in prompts:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Prompt: '{prompt}'\")\n",
        "    print('='*60)\n",
        "\n",
        "    print(\"\\n--- Greedy Decoding ---\")\n",
        "    greedy_text = generate_greedy(\n",
        "        char_model, prompt, 200,\n",
        "        char_dataset.char_to_idx, char_dataset.idx_to_char, device\n",
        "    )\n",
        "    print(greedy_text)\n",
        "\n",
        "    for temp in [0.5, 1.0, 2.0]:\n",
        "        print(f\"\\n--- Temperature {temp} ---\")\n",
        "        temp_text = generate_with_temperature(\n",
        "            char_model, prompt, 200, temp,\n",
        "            char_dataset.char_to_idx, char_dataset.idx_to_char, device\n",
        "        )\n",
        "        print(temp_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QpE0X_HJQkS"
      },
      "source": [
        "---\n",
        "\n",
        "# Stage 2: Word-Level Language Modeling & Theatrical Chat Interface\n",
        "\n",
        "In this stage, you will move from **character-level** to **word-level** language modeling. You will:\n",
        "\n",
        "- Build a word-level vocabulary and dataset from Shakespeare's works\n",
        "- Implement and train a **word-level RNN language model**\n",
        "- Construct a simple **theatrical chat interface** that simulates dialog between characters\n",
        "- Create a **turn-aware** model with special end-of-turn tokens\n",
        "\n",
        "---\n",
        "\n",
        "## Exercise 2.1 – Word-Level Vocabulary and Sequential Dataset\n",
        "\n",
        "### Description\n",
        "\n",
        "You will construct a **word-level representation** of Shakespeare's text and prepare a dataset for **next-word prediction**.\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "After completing this exercise, you should be able to:\n",
        "- Extend a tokenizer for word-level modeling\n",
        "- Build a **word vocabulary** with frequency cutoffs\n",
        "- Handle out-of-vocabulary words with `<UNK>` token\n",
        "- Prepare sliding-window input-target pairs for next-word prediction\n",
        "\n",
        "### Task\n",
        "\n",
        "Instead of a character-based dataset, we now transition to a word-based dataset. The methods are the same as before, except we now split the text into words and limit ourselves to a vocabulary of a fixed size `vocab_size`, which should consist of the most common tokens in the corpus (hint: use a Counter). The vocabulary should also contain an `unk_token = \"<UNK>\"`, which is our stand-in for any future words we do not know (either rare tokens from this corpus or other texts).\n",
        "\n",
        "1. Build the vocabulary. **(0.5 points)**\n",
        "2. Build `<UNK>` handling. **(0.5 points)**\n",
        "3. Build the rest of the dataset. **(0.5 points)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZgTNZqYJQkS"
      },
      "outputs": [],
      "source": [
        "# Exercise 2.1: Word-Level Vocabulary and Sequential Dataset\n",
        "\n",
        "# You can reuse the tokenize function from Exercise 0.2 (already defined above)\n",
        "\n",
        "class WordDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A PyTorch Dataset for word-level language modeling.\n",
        "\n",
        "    Creates input-target pairs using a sliding window over tokenized text.\n",
        "    Handles vocabulary building and out-of-vocabulary words.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, text: str, seq_len: int, vocab_size: int = 30000):\n",
        "        \"\"\"\n",
        "        Initialize the dataset.\n",
        "\n",
        "        Args:\n",
        "            text: The full text as a string\n",
        "            seq_len: Length of each sequence (number of words)\n",
        "            vocab_size: Maximum vocabulary size (most frequent words)\n",
        "        \"\"\"\n",
        "        # Task 2.1: START STUDENT CODE\n",
        "\n",
        "        # HINT:\n",
        "        # 1. Store seq_len and tokenize the text\n",
        "        # 2. Build vocabulary: count token frequencies, keep most common up to vocab_size\n",
        "        # 3. Create bidirectional mappings with an <UNK> token for OOV words\n",
        "        # 4. Encode all tokens to indices\n",
        "        pass\n",
        "\n",
        "        # Task 2.1: END STUDENT CODE\n",
        "\n",
        "    def __len__(self):\n",
        "        # Task 2.2: START STUDENT CODE\n",
        "        \"\"\"Number of samples available.\"\"\"\n",
        "\n",
        "        # HINT: Similar to CharDataset, return the number of valid sequences\n",
        "        pass\n",
        "\n",
        "        # Task 2.2: END STUDENT CODE\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Task 2.3: START STUDENT CODE\n",
        "        \"\"\"Get a single sample (input, target pair).\"\"\"\n",
        "\n",
        "        # HINT: Implement sliding window for next-word prediction (same as character level)\n",
        "        pass\n",
        "\n",
        "        # Task 2.3: END STUDENT CODE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "palIme7xYwVP"
      },
      "outputs": [],
      "source": [
        "# Load the full corpus for word-level modeling\n",
        "print(\"--- Exercise 2.1: Word-Level Dataset ---\")\n",
        "print(\"\\nLoading full Shakespeare corpus...\")\n",
        "\n",
        "all_text = []\n",
        "work_files = sorted([f for f in os.listdir(WORKS_DIR) if f.endswith('.txt')])\n",
        "# use a selective works if training takes too long\n",
        "work_files = [f for f in work_files if \"romeo\" in f]\n",
        "\n",
        "print(f\"Found {len(work_files)} works\")\n",
        "\n",
        "for filename in work_files:\n",
        "    with open(os.path.join(WORKS_DIR, filename), 'r', encoding='utf-8') as f:\n",
        "        all_text.append(f.read())\n",
        "\n",
        "full_corpus = \"\\n\".join(all_text)\n",
        "print(f\"Total corpus length: {len(full_corpus):,} characters\")\n",
        "\n",
        "# Create dataset\n",
        "WORD_SEQ_LEN = 100  # 100 words of context\n",
        "WORD_BATCH_SIZE = 64\n",
        "WORD_VOCAB_SIZE = 30000\n",
        "\n",
        "word_dataset = WordDataset(full_corpus, WORD_SEQ_LEN, vocab_size=WORD_VOCAB_SIZE)\n",
        "word_dataloader = DataLoader(word_dataset, batch_size=WORD_BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Verify\n",
        "inputs, targets = next(iter(word_dataloader))\n",
        "print(f\"\\nBatch shapes: Input {inputs.shape}, Target {targets.shape}\")\n",
        "\n",
        "# Decode sample\n",
        "sample_input = ' '.join([word_dataset.idx_to_word[i.item()] for i in inputs[0][:20]])\n",
        "print(f\"\\nSample input (first 20 words): {sample_input}...\")\n",
        "\n",
        "print(f\"\\n--- Design Notes ---\")\n",
        "print(f\"seq_len={WORD_SEQ_LEN}: Longer context window for word-level modeling\")\n",
        "print(f\"vocab_size={WORD_VOCAB_SIZE}: Accommodates Shakespeare's vocabulary\")\n",
        "print(f\"<UNK> handling: Rare words mapped to single UNK token\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEEDPce4JQkS"
      },
      "source": [
        "---\n",
        "\n",
        "## Exercise 2.2 – Word-Level RNN Language Model in PyTorch\n",
        "\n",
        "### Description\n",
        "\n",
        "You will implement and train a **word-level RNN-based language model** using the dataset from Exercise 2.1. The model will learn to predict the next word given a sequence of preceding words.\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "After completing this exercise, you should be able to:\n",
        "- Define a word-level recurrent neural language model\n",
        "- Use larger embedding dimensions appropriate for word-level modeling\n",
        "- Train with cross-entropy loss on next-word prediction\n",
        "\n",
        "### Tasks\n",
        "\n",
        "1. Define the `WordRNNLM` model class (similar to CharRNNLM but for words). **(1 point)**\n",
        "2. Train for multiple epochs on the full corpus and save model checkpoints.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmlCO9-nJQkS"
      },
      "outputs": [],
      "source": [
        "# Exercise 2.2: Word-Level RNN Language Model\n",
        "\n",
        "\n",
        "class WordRNNLM(nn.Module):\n",
        "    \"\"\"\n",
        "    Word-level RNN Language Model.\n",
        "\n",
        "    Similar architecture to CharRNNLM but with:\n",
        "    - Larger embedding dimensions (words need more representation capacity)\n",
        "    - Multiple RNN layers for better modeling\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size: int, emb_dim: int, hidden_size: int, num_layers: int = 3):\n",
        "        \"\"\"\n",
        "        Initialize the model.\n",
        "\n",
        "        Args:\n",
        "            vocab_size: Number of words in vocabulary\n",
        "            emb_dim: Dimension of word embeddings\n",
        "            hidden_size: Number of hidden units in RNN\n",
        "            num_layers: Number of stacked RNN layers\n",
        "        \"\"\"\n",
        "        # Task 2.4: START STUDENT CODE\n",
        "\n",
        "        # HINT: Build a word-level RNN model (similar to CharRNNLM but with num_layers param)\n",
        "        pass\n",
        "\n",
        "        # Task 2.4: END STUDENT CODE\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch, seq_len)\n",
        "            hidden: Optional initial hidden state\n",
        "\n",
        "        Returns:\n",
        "            logits: Output logits of shape (batch, seq_len, vocab_size)\n",
        "            hidden: Final hidden state\n",
        "        \"\"\"\n",
        "        # Task 2.5: START STUDENT CODE\n",
        "\n",
        "        # HINT: Forward pass through RNN (same as CharRNNLM)\n",
        "        pass\n",
        "\n",
        "        # Task 2.5: END STUDENT CODE\n",
        "\n",
        "print(\"--- Exercise 2.2: Word-Level RNN Model ---\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_hZTPptJQkS"
      },
      "outputs": [],
      "source": [
        "# Exercise 2.2 (continued): Training Word RNN\n",
        "\n",
        "# Model hyperparameters - larger than char model\n",
        "WORD_EMB_DIM = 300\n",
        "WORD_HIDDEN_SIZE = 512\n",
        "WORD_NUM_LAYERS = 3\n",
        "WORD_EPOCHS = 3  # Fewer epochs due to larger corpus\n",
        "WORD_LR = 0.001\n",
        "\n",
        "# Create model\n",
        "word_model = WordRNNLM(\n",
        "    vocab_size=word_dataset.vocab_size,\n",
        "    emb_dim=WORD_EMB_DIM,\n",
        "    hidden_size=WORD_HIDDEN_SIZE,\n",
        "    num_layers=WORD_NUM_LAYERS\n",
        ").to(device)\n",
        "\n",
        "print(f\"Model created with {sum(p.numel() for p in word_model.parameters()):,} parameters\")\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(word_model.parameters(), lr=WORD_LR)\n",
        "\n",
        "# Training loop\n",
        "total_batches = len(word_dataloader)\n",
        "word_model.train()\n",
        "\n",
        "for epoch in range(WORD_EPOCHS):\n",
        "    total_loss = 0\n",
        "    for i, (inputs, targets) in enumerate(word_dataloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits, _ = word_model(inputs)\n",
        "        loss = criterion(logits.view(-1, word_dataset.vocab_size), targets.view(-1))\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(word_model.parameters(), 5)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Progress logging\n",
        "        if i % 200 == 0:\n",
        "            pct = (i / total_batches) * 100\n",
        "            print(f\"Epoch {epoch+1}/{WORD_EPOCHS} | Batch {i}/{total_batches} ({pct:.1f}%) | Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / len(word_dataloader)\n",
        "    print(f\"Epoch {epoch+1}/{WORD_EPOCHS} Complete | Avg Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Save model\n",
        "word_model_path = \"word_rnn_model.pth\"\n",
        "torch.save({\n",
        "    'model_state_dict': word_model.state_dict(),\n",
        "    'vocab_size': word_dataset.vocab_size,\n",
        "    'emb_dim': WORD_EMB_DIM,\n",
        "    'hidden_size': WORD_HIDDEN_SIZE,\n",
        "    'num_layers': WORD_NUM_LAYERS,\n",
        "    'word_to_idx': word_dataset.word_to_idx,\n",
        "    'idx_to_word': word_dataset.idx_to_word,\n",
        "}, word_model_path)\n",
        "print(f\"\\nModel saved to {word_model_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujK9u-hdJQkS"
      },
      "source": [
        "---\n",
        "\n",
        "## Exercise 2.3 – Theatrical Chat Interface with a Word-Level RNN\n",
        "\n",
        "### Description\n",
        "\n",
        "In this exercise, you will use your trained **word-level RNN language model** to build a simple **theatrical chat interface**. The model will be prompted with a speaker name and dialog, then continue the text in Shakespearean style.\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "After completing this exercise, you should be able to:\n",
        "- Use a word-level language model for **prompt-based generation**\n",
        "- Design a simple **chat-style interface** around a language model\n",
        "- Control text generation via temperature sampling at the word level\n",
        "\n",
        "### Tasks\n",
        "\n",
        "1. Implement a word-level generation function that detokenizes the generated text after generation. **(1 point)**\n",
        "2. Test with different theatrical prompts (ROMEO., JULIET., etc.) and different temperatures. **(0.5 points)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-I5a2L3YJQkS"
      },
      "outputs": [],
      "source": [
        "# Exercise 2.3: Theatrical Chat Interface\n",
        "\n",
        "def detokenize(tokens: list) -> str:\n",
        "    \"\"\"\n",
        "    Convert a list of tokens back into readable text.\n",
        "    Handles punctuation attachment (no space before punctuation).\n",
        "\n",
        "    Args:\n",
        "        tokens: List of token strings\n",
        "\n",
        "    Returns:\n",
        "        Reconstructed text string\n",
        "    \"\"\"\n",
        "    text = \"\"\n",
        "    for t in tokens:\n",
        "        # Attach punctuation without leading space\n",
        "        if t in [\".\", \",\", \"?\", \"!\", \":\", \";\", \"'\"] or t.startswith(\"'\"):\n",
        "            if text:\n",
        "                text = text.rstrip() + t + \" \"\n",
        "            else:\n",
        "                text += t + \" \"\n",
        "        else:\n",
        "            text += t + \" \"\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def generate_words(model, start_text: str, max_tokens: int, temperature: float,\n",
        "                   word_to_idx: dict, idx_to_word: dict, device='cpu') -> str:\n",
        "    \"\"\"\n",
        "    Generate text at the word level with temperature sampling.\n",
        "\n",
        "    Args:\n",
        "        model: Trained WordRNNLM model\n",
        "        start_text: Initial prompt text\n",
        "        max_tokens: Maximum number of words to generate\n",
        "        temperature: Sampling temperature\n",
        "        word_to_idx: Word to index mapping\n",
        "        idx_to_word: Index to word mapping\n",
        "        device: Device to run on\n",
        "\n",
        "    Returns:\n",
        "        Generated text string (including prompt)\n",
        "    \"\"\"\n",
        "    # Task 2.6: START STUDENT CODE\n",
        "\n",
        "    # HINT: Word-level generation is similar to character-level but:\n",
        "    # 1. Tokenize the prompt using the tokenize() function\n",
        "    # 2. Map tokens to indices (handle UNK)\n",
        "    # 3. Generate words with temperature sampling (like Task 1.9)\n",
        "    # 4. Detokenize the result (reconstruct readable text with proper spacing)\n",
        "    pass\n",
        "\n",
        "    # Task 2.6: END STUDENT CODE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pYmiFWfYwVQ"
      },
      "outputs": [],
      "source": [
        "# Test theatrical generation\n",
        "print(\"--- Exercise 2.3: Theatrical Chat Interface ---\")\n",
        "\n",
        "examples = [\n",
        "    (\"ROMEO\", \"My heart is heavy with unspoken words.\"),\n",
        "    (\"JULIET\", \"Good even, my lord. Why art thou troubled?\"),\n",
        "    (\"HAMLET\", \"To be, or not to be, that is the question.\")\n",
        "]\n",
        "\n",
        "for speaker, line in examples:\n",
        "    prompt = f\"{speaker}.\\n{line}\\n\"\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"{speaker}: {line}\")\n",
        "    print('='*60)\n",
        "\n",
        "    for temp in [0.5, 0.8, 1.2]:\n",
        "        response = generate_words(\n",
        "            word_model, prompt, 50, temp,\n",
        "            word_dataset.word_to_idx, word_dataset.idx_to_word, device\n",
        "        )\n",
        "        print(f\"\\n[Temperature {temp}]\")\n",
        "        print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7nmjPezJQkS"
      },
      "source": [
        "---\n",
        "\n",
        "## Exercise 2.4 – Turn-Based Modeling with Special Tokens\n",
        "\n",
        "### Description\n",
        "\n",
        "To build a realistic theatrical chat interface, the model needs to understand when a speaker's turn ends. In this exercise, you will create a specialized dataset that inserts a special **End-of-Turn** token (`<EOS>`) before every speaker change.\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "- Preprocess text to explicitly model dialog structure (turns)\n",
        "- Use special tokens (`<EOS>`) to control generation length\n",
        "- Train a language model that learns to stop generating at appropriate times\n",
        "\n",
        "### Key Insight\n",
        "\n",
        "By training with `<EOS>` markers before speaker changes, the model learns:\n",
        "1. When to stop generating (predict `<EOS>`)\n",
        "2. The natural rhythm of theatrical dialogue\n",
        "\n",
        "### Tasks\n",
        "\n",
        "1. Implement a `insert_turn_markers()` to add `<EOS>` before speaker names. **(1 point)**\n",
        "2. Create a `TurnDataset` with the modified corpus and train a turn-aware model. **(1 point)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UwEeuDtJQkT"
      },
      "outputs": [],
      "source": [
        "# Exercise 2.4: Turn-Based Modeling with Special Tokens\n",
        "\n",
        "def insert_turn_markers(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Insert <EOS> markers before speaker names in the text.\n",
        "\n",
        "    Speaker detection heuristic:\n",
        "    - Lines that are predominantly uppercase\n",
        "    - Short (< 30 characters)\n",
        "    - Not empty\n",
        "\n",
        "    Args:\n",
        "        text: Original text\n",
        "\n",
        "    Returns:\n",
        "        Text with <EOS> markers inserted before speaker names\n",
        "    \"\"\"\n",
        "    # Task 2.7: START STUDENT CODE\n",
        "\n",
        "    # HINT: Detect speaker names and insert <EOS> markers\n",
        "    # 1. Split text into lines\n",
        "    # 2. For each line, detect if it's a speaker name:\n",
        "    #    - Non-empty, short (< 30 chars), mostly uppercase\n",
        "    # 3. If speaker: insert \"<EOS>\" before the line\n",
        "    # 4. Rejoin lines\n",
        "    pass\n",
        "\n",
        "    # Task 2.7: END STUDENT CODE\n",
        "\n",
        "\n",
        "class TurnDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset with turn markers for dialogue modeling.\n",
        "\n",
        "    Similar to WordDataset but:\n",
        "    - Preprocesses text with <EOS> markers\n",
        "    - Ensures <EOS> token is in vocabulary\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, text: str, seq_len: int, vocab_size: int = 30000):\n",
        "        # Task 2.8: START STUDENT CODE\n",
        "\n",
        "        # HINT: Similar to WordDataset but with turn markers:\n",
        "        # 1. Store seq_len\n",
        "        # 2. Insert <EOS> markers before speakers (use insert_turn_markers)\n",
        "        # 3. Replace <EOS> with \"eos_marker\" so tokenizer handles it properly\n",
        "        # 4. Tokenize and build vocabulary (reserve space for UNK and eos_marker)\n",
        "        # 5. Ensure both UNK and EOS tokens are in vocabulary\n",
        "        # 6. Encode all tokens\n",
        "        pass\n",
        "\n",
        "        # Task 2.8: END STUDENT CODE\n",
        "\n",
        "    def __len__(self):\n",
        "        # Task 2.9: START STUDENT CODE\n",
        "\n",
        "        # HINT: Same as WordDataset\n",
        "        pass\n",
        "\n",
        "        # Task 2.9: END STUDENT CODE\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Task 2.10: START STUDENT CODE\n",
        "\n",
        "        # HINT: Same as WordDataset\n",
        "        pass\n",
        "\n",
        "        # Task 2.10: END STUDENT CODE\n",
        "\n",
        "# Create turn-aware dataset\n",
        "print(\"--- Exercise 2.4: Turn-Based Dataset ---\")\n",
        "\n",
        "turn_dataset = TurnDataset(full_corpus, WORD_SEQ_LEN, vocab_size=WORD_VOCAB_SIZE)\n",
        "turn_dataloader = DataLoader(turn_dataset, batch_size=WORD_BATCH_SIZE, shuffle=True)\n",
        "\n",
        "print(f\"\\nTurn Dataset Stats:\")\n",
        "print(f\"  Vocabulary size: {turn_dataset.vocab_size}\")\n",
        "print(f\"  EOS token index: {turn_dataset.word_to_idx[turn_dataset.eos_token]}\")\n",
        "print(f\"  Number of samples: {len(turn_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNLV0fxgJQkT"
      },
      "outputs": [],
      "source": [
        "# Exercise 2.4 (continued): Training Turn-Aware Model\n",
        "\n",
        "print(\"--- Training Turn-Aware RNN ---\")\n",
        "\n",
        "# Create turn-aware model (same architecture, different vocab/data)\n",
        "turn_model = WordRNNLM(\n",
        "    vocab_size=turn_dataset.vocab_size,\n",
        "    emb_dim=WORD_EMB_DIM,\n",
        "    hidden_size=WORD_HIDDEN_SIZE,\n",
        "    num_layers=WORD_NUM_LAYERS\n",
        ").to(device)\n",
        "\n",
        "print(f\"Turn model created with {sum(p.numel() for p in turn_model.parameters()):,} parameters\")\n",
        "\n",
        "# Training setup\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(turn_model.parameters(), lr=WORD_LR)\n",
        "\n",
        "# Training loop\n",
        "total_batches = len(turn_dataloader)\n",
        "turn_model.train()\n",
        "\n",
        "TURN_EPOCHS = 3\n",
        "\n",
        "for epoch in range(TURN_EPOCHS):\n",
        "    total_loss = 0\n",
        "    for i, (inputs, targets) in enumerate(turn_dataloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits, _ = turn_model(inputs)\n",
        "        loss = criterion(logits.view(-1, turn_dataset.vocab_size), targets.view(-1))\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(turn_model.parameters(), 5)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if i % 200 == 0:\n",
        "            pct = (i / total_batches) * 100\n",
        "            print(f\"Epoch {epoch+1}/{TURN_EPOCHS} | Batch {i}/{total_batches} ({pct:.1f}%) | Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / len(turn_dataloader)\n",
        "    print(f\"Epoch {epoch+1}/{TURN_EPOCHS} Complete | Avg Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Save turn-aware model\n",
        "turn_model_path = \"turn_rnn_model.pth\"\n",
        "torch.save({\n",
        "    'model_state_dict': turn_model.state_dict(),\n",
        "    'vocab_size': turn_dataset.vocab_size,\n",
        "    'emb_dim': WORD_EMB_DIM,\n",
        "    'hidden_size': WORD_HIDDEN_SIZE,\n",
        "    'num_layers': WORD_NUM_LAYERS,\n",
        "    'word_to_idx': turn_dataset.word_to_idx,\n",
        "    'idx_to_word': turn_dataset.idx_to_word,\n",
        "    'eos_token': turn_dataset.eos_token,\n",
        "}, turn_model_path)\n",
        "print(f\"\\nTurn-aware model saved to {turn_model_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtouOJxlYwVS"
      },
      "source": [
        "---\n",
        "\n",
        "## Stage 2.5 – Theatrical Chat Interface (Model Comparison)\n",
        "\n",
        "### Description\n",
        "\n",
        "You can now use your trained models to build a chat interface that supports **both** the standard Word-RNN (from Exercise 2.2) and the Turn-Aware RNN (from Exercise 2.4).\n",
        "\n",
        "### Key Differences\n",
        "\n",
        "| Model | Generation Behavior |\n",
        "|-------|---------------------|\n",
        "| Standard Word-RNN | Generates exactly `max_tokens` words |\n",
        "| Turn-Aware RNN | Stops when `<EOS>` is generated |\n",
        "\n",
        "### Tasks\n",
        "\n",
        "No tasks, just execute the code below and play around with it, to get a feeling for the performance of the two variants.\n",
        "\n",
        "**The `generate_with_eos` function will be useful in the next tasks - you can and should use it, and should familiarize yourself with what it does.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uexn6eKWYwVS"
      },
      "outputs": [],
      "source": [
        "# Exercise 2.5: Theatrical Chat Interface (Model Comparison)\n",
        "\n",
        "def generate_with_eos(model, start_text: str, max_tokens: int, temperature: float,\n",
        "                      word_to_idx: dict, idx_to_word: dict, eos_token: str = None,\n",
        "                      device='cpu') -> str:\n",
        "    \"\"\"\n",
        "    Generate text with optional EOS stopping.\n",
        "\n",
        "    Args:\n",
        "        model: Trained language model\n",
        "        start_text: Initial prompt\n",
        "        max_tokens: Maximum tokens to generate\n",
        "        temperature: Sampling temperature\n",
        "        word_to_idx: Word to index mapping\n",
        "        idx_to_word: Index to word mapping\n",
        "        eos_token: If provided, stop when this token is generated\n",
        "        device: Device to run on\n",
        "\n",
        "    Returns:\n",
        "        Generated text (excluding prompt tokens)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    hidden = None\n",
        "\n",
        "    # Preprocess prompt - handle <eos> placeholder\n",
        "    text_to_process = start_text.lower()\n",
        "    if eos_token and \"<eos>\" in text_to_process:\n",
        "        text_to_process = text_to_process.replace(\"<eos>\", eos_token)\n",
        "\n",
        "    tokens = tokenize(text_to_process)\n",
        "    unk_idx = word_to_idx.get(\"<UNK>\", 0)\n",
        "\n",
        "    input_indices = [word_to_idx.get(t, unk_idx) for t in tokens]\n",
        "    input_seq = torch.tensor(input_indices, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    generated_tokens = []\n",
        "\n",
        "    # Get EOS index if applicable\n",
        "    eos_idx = word_to_idx.get(eos_token, -1) if eos_token else -1\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits, hidden = model(input_seq, hidden)\n",
        "        last_logits = logits[:, -1, :]\n",
        "\n",
        "        for _ in range(max_tokens):\n",
        "            if temperature <= 0:\n",
        "                idx = torch.argmax(last_logits, dim=-1).item()\n",
        "            else:\n",
        "                probs = F.softmax(last_logits / temperature, dim=-1)\n",
        "                idx = torch.multinomial(probs, 1).item()\n",
        "\n",
        "            # Check for EOS\n",
        "            if eos_token and idx == eos_idx:\n",
        "                break\n",
        "\n",
        "            word = idx_to_word.get(idx, \"<UNK>\")\n",
        "            generated_tokens.append(word)\n",
        "\n",
        "            input_seq = torch.tensor([[idx]], dtype=torch.long).to(device)\n",
        "            logits, hidden = model(input_seq, hidden)\n",
        "            last_logits = logits[:, -1, :]\n",
        "\n",
        "    return detokenize(generated_tokens)\n",
        "\n",
        "# Compare models\n",
        "print(\"--- Exercise 2.5: Model Comparison ---\")\n",
        "\n",
        "examples = [\n",
        "    (\"ROMEO\", \"My heart is heavy with unspoken words.\"),\n",
        "    (\"JULIET\", \"Good even, my lord. Why art thou troubled?\"),\n",
        "]\n",
        "\n",
        "for speaker, line in examples:\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"PROMPT: {speaker}: {line}\")\n",
        "    print('='*70)\n",
        "\n",
        "    # Standard model (fixed length)\n",
        "    standard_prompt = f\"{speaker}.\\n{line}\\n\"\n",
        "    standard_response = generate_with_eos(\n",
        "        word_model, standard_prompt, 50, 0.8,\n",
        "        word_dataset.word_to_idx, word_dataset.idx_to_word,\n",
        "        eos_token=None, device=device\n",
        "    )\n",
        "    print(f\"\\n[Standard Word-RNN (50 tokens)]\")\n",
        "    print(standard_response[:200] + \"...\" if len(standard_response) > 200 else standard_response)\n",
        "\n",
        "    # Turn-aware model (EOS stopping)\n",
        "    turn_prompt = f\"<eos>\\n{speaker}.\\n{line}\\n<eos>\\n\"\n",
        "    turn_response = generate_with_eos(\n",
        "        turn_model, turn_prompt, 200, 0.8,\n",
        "        turn_dataset.word_to_idx, turn_dataset.idx_to_word,\n",
        "        eos_token=turn_dataset.eos_token, device=device\n",
        "    )\n",
        "    print(f\"\\n[Turn-Aware RNN (EOS stopping)]\")\n",
        "    print(turn_response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0ih4uoNJQkT"
      },
      "source": [
        "---\n",
        "\n",
        "# Stage 3: Word-Level LSTM & RNN–LSTM Comparison\n",
        "\n",
        "In this final stage, you will extend your word-level language model by replacing the RNN with an **LSTM**. You will:\n",
        "\n",
        "- Implement and train a **word-level LSTM language model**\n",
        "- Plug the LSTM into your existing **theatrical chat interface**\n",
        "- Qualitatively compare the behavior of **RNN** vs **LSTM** models\n",
        "\n",
        "## Why LSTM?\n",
        "\n",
        "Long Short-Term Memory (LSTM) networks address the **vanishing gradient problem** in standard RNNs:\n",
        "\n",
        "| Feature | RNN | LSTM |\n",
        "|---------|-----|------|\n",
        "| Memory | Short-term only | Long and short-term |\n",
        "| Gradient flow | Degrades over long sequences | Gates preserve gradients |\n",
        "| Training | Faster per step | More stable |\n",
        "| Parameters | Fewer | ~4x more (3 gates + cell) |\n",
        "\n",
        "---\n",
        "\n",
        "## Exercise 3.1 – Word-Level LSTM Language Model in PyTorch\n",
        "\n",
        "### Description\n",
        "\n",
        "You will modify your word-level language model by replacing the RNN layer with an LSTM.\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "- Implement a **word-level LSTM-based language model**\n",
        "- Handle LSTM's dual hidden state `(h, c)`\n",
        "- Train on the same turn-aware dataset for fair comparison\n",
        "\n",
        "### Tasks\n",
        "\n",
        "1. Define `WordLSTMLM` class with `nn.LSTM`. It has to fulfill the same conditions as normally, except this time, the model should return both the prediction and the LSTM's hidden state. **(1 point)**\n",
        "2. Train on the same dataset as the RNN and compare training behavior in terms of loss, stability, etc. **(1 point)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3KWm_YCJQkT"
      },
      "outputs": [],
      "source": [
        "# Exercise 3.1: Word-Level LSTM Language Model\n",
        "\n",
        "class WordLSTMLM(nn.Module):\n",
        "    \"\"\"\n",
        "    Word-level LSTM Language Model.\n",
        "\n",
        "    Key difference from RNN:\n",
        "    - Uses nn.LSTM instead of nn.RNN\n",
        "    - Hidden state is a tuple (h_n, c_n) where:\n",
        "      - h_n: hidden state (same as RNN)\n",
        "      - c_n: cell state (LSTM's long-term memory)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size: int, emb_dim: int, hidden_size: int, num_layers: int = 3):\n",
        "        \"\"\"\n",
        "        Initialize the LSTM model.\n",
        "\n",
        "        Args:\n",
        "            vocab_size: Number of words in vocabulary\n",
        "            emb_dim: Dimension of word embeddings\n",
        "            hidden_size: Number of hidden units in LSTM\n",
        "            num_layers: Number of stacked LSTM layers\n",
        "        \"\"\"\n",
        "        # Task 3.1: START STUDENT CODE\n",
        "\n",
        "        # HINT: Build a word-level LSTM model (same as WordRNNLM but use nn.LSTM)\n",
        "        pass\n",
        "\n",
        "        # Task 3.1: END STUDENT CODE\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch, seq_len)\n",
        "            hidden: Optional initial hidden state tuple (h_0, c_0)\n",
        "\n",
        "        Returns:\n",
        "            logits: Output logits of shape (batch, seq_len, vocab_size)\n",
        "            hidden: Final hidden state tuple (h_n, c_n)\n",
        "        \"\"\"\n",
        "        # Task 3.2: START STUDENT CODE\n",
        "\n",
        "        # HINT: Forward pass through LSTM (same as RNN forward pass)\n",
        "        pass\n",
        "\n",
        "        # Task 3.2: END STUDENT CODE\n",
        "\n",
        "\n",
        "print(\"--- Exercise 3.1: Word-Level LSTM Model ---\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKo0P-WXJQkT"
      },
      "outputs": [],
      "source": [
        "# Exercise 3.1 (continued): Training LSTM Model\n",
        "\n",
        "# Create LSTM model (same hyperparameters as RNN for fair comparison)\n",
        "lstm_model = WordLSTMLM(\n",
        "    vocab_size=turn_dataset.vocab_size,\n",
        "    emb_dim=WORD_EMB_DIM,\n",
        "    hidden_size=WORD_HIDDEN_SIZE,\n",
        "    num_layers=WORD_NUM_LAYERS\n",
        ").to(device)\n",
        "\n",
        "print(f\"LSTM model created with {sum(p.numel() for p in lstm_model.parameters()):,} parameters\")\n",
        "print(f\"(Compare to RNN: ~{sum(p.numel() for p in turn_model.parameters()):,} parameters)\")\n",
        "\n",
        "# Training setup\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(lstm_model.parameters(), lr=WORD_LR)\n",
        "\n",
        "# Training loop\n",
        "total_batches = len(turn_dataloader)\n",
        "lstm_model.train()\n",
        "\n",
        "LSTM_EPOCHS = 3\n",
        "\n",
        "for epoch in range(LSTM_EPOCHS):\n",
        "    total_loss = 0\n",
        "    for i, (inputs, targets) in enumerate(turn_dataloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits, _ = lstm_model(inputs)\n",
        "        loss = criterion(logits.view(-1, turn_dataset.vocab_size), targets.view(-1))\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(lstm_model.parameters(), 5)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if i % 200 == 0:\n",
        "            pct = (i / total_batches) * 100\n",
        "            print(f\"Epoch {epoch+1}/{LSTM_EPOCHS} | Batch {i}/{total_batches} ({pct:.1f}%) | Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / len(turn_dataloader)\n",
        "    print(f\"Epoch {epoch+1}/{LSTM_EPOCHS} Complete | Avg Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Save LSTM model\n",
        "lstm_model_path = \"word_lstm_model.pth\"\n",
        "torch.save({\n",
        "    'model_state_dict': lstm_model.state_dict(),\n",
        "    'vocab_size': turn_dataset.vocab_size,\n",
        "    'emb_dim': WORD_EMB_DIM,\n",
        "    'hidden_size': WORD_HIDDEN_SIZE,\n",
        "    'num_layers': WORD_NUM_LAYERS,\n",
        "    'word_to_idx': turn_dataset.word_to_idx,\n",
        "    'idx_to_word': turn_dataset.idx_to_word,\n",
        "    'eos_token': turn_dataset.eos_token,\n",
        "}, lstm_model_path)\n",
        "print(f\"\\nLSTM model saved to {lstm_model_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MXouAiAJQkT"
      },
      "source": [
        "---\n",
        "\n",
        "## Exercise 3.2 – LSTM Chat Interface and RNN–LSTM Comparison\n",
        "\n",
        "### Description\n",
        "\n",
        "You will now plug your **word-level LSTM language model** into the theatrical chat interface and compare its behavior to the **word-level RNN** using identical prompts and settings.\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "- Use a word-level LSTM for **prompt-based generation**\n",
        "- Compare outputs of RNN vs LSTM models\n",
        "- Reflect on advantages and limitations of both architectures\n",
        "\n",
        "### Tasks\n",
        "\n",
        "Plug the LSTM model into the chat interface you made and compare to the RNN chat generation, using the following comparison criteria **(1 point)**:\n",
        "\n",
        "1. **Coherence**: Does the text make grammatical sense?\n",
        "2. **Dialog structure**: Does it follow speaker patterns?\n",
        "3. **Repetition**: Does one model repeat itself more?\n",
        "4. **Creativity**: Which produces more varied outputs?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVlFecyyJQkT"
      },
      "outputs": [],
      "source": [
        "# Exercise 3.2: RNN vs LSTM Comparison\n",
        "\n",
        "print(\"--- Exercise 3.2: RNN vs LSTM Comparison ---\")\n",
        "\n",
        "# Test prompts\n",
        "comparison_prompts = [\n",
        "    (\"ROMEO\", \"My heart is heavy with unspoken words.\", \"JULIET\"),\n",
        "    (\"JULIET\", \"Good even, my lord. Why art thou troubled?\", \"ROMEO\"),\n",
        "    (\"HAMLET\", \"To be, or not to be, I ask again.\", \"HORATIO\"),\n",
        "]\n",
        "\n",
        "for user_speaker, user_line, model_speaker in comparison_prompts:\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"{user_speaker}: {user_line}\")\n",
        "    print(f\"(Response from: {model_speaker})\")\n",
        "    print('='*70)\n",
        "\n",
        "    # Construct prompts\n",
        "    prompt = f\"<eos>\\n{user_speaker}.\\n{user_line}\\n<eos>\\n{model_speaker}.\\n\"\n",
        "\n",
        "    # RNN response\n",
        "    # Task 3.3: START STUDENT CODE\n",
        "\n",
        "    # HINT: Generate rnn_response using the turn_model (RNN) with the prompt\n",
        "    pass\n",
        "\n",
        "    # Task 3.3: END STUDENT CODE\n",
        "    print(f\"\\n[RNN] {model_speaker}:\")\n",
        "    print(rnn_response)\n",
        "\n",
        "    # LSTM response\n",
        "    # Task 3.4: START STUDENT CODE\n",
        "\n",
        "    # HINT: Generate lstm_response using the lstm_model (LSTM) with the same prompt\n",
        "    pass\n",
        "\n",
        "    # Task 3.4: END STUDENT CODE\n",
        "    print(f\"\\n[LSTM] {model_speaker}:\")\n",
        "    print(lstm_response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9Ydx96iJQkT"
      },
      "source": [
        "---\n",
        "\n",
        "# 🎭 Congratulations!\n",
        "\n",
        "You have successfully completed all exercises in this NLP workshop. Here's what you've accomplished:\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "1. **Data Preprocessing Matters**: Tokenization, normalization, and special tokens significantly impact model behavior.\n",
        "\n",
        "2. **Architecture Choices**: RNNs are simple but struggle with long sequences; LSTMs address this with gating mechanisms.\n",
        "\n",
        "3. **Temperature Sampling**: Controls the creativity-coherence tradeoff in generation.\n",
        "\n",
        "4. **Turn-Aware Training**: Special tokens help models learn dialogue structure.\n",
        "\n",
        "## Further Exploration\n",
        "\n",
        "- Try different hyperparameters (hidden size, layers, learning rate)\n",
        "- Try different combinations of shakespeare's works for training\n",
        "- Try making LSTMs work for turn-based conversations"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}