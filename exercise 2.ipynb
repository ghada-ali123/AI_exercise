{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOhcVY71m0liKZAfIzR/GKA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ghada-ali123/AI_exercise/blob/main/exercise%202.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Task 1\n",
        " If we impute empty data points in machine learning, it would guess these points according to the data points around. this is immaginary way to get the missing points due to the mean or median (for numerical data) this means there is a possibility of error in output.\n",
        "In some cases, it is valuable to make imputation for missing data points because dropping raw with missing data wouid reduce the data set size and led to bias to remaining data. therefore, it is crucial to make imputation in preprocessing pipelines to ensure the system able to handle with missing points."
      ],
      "metadata": {
        "id": "-x7fBuNi7awH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "task 2\n",
        "the logistic regression model is generalized better than decision tree model. it considered a siple model with lower variances and\n",
        "regularity prevent overfitting. On the other hand, the tree decission can fit noise and it has a high probability of overfitting and\n",
        "split on a small details\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9krCClxm7-MY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 3\n",
        " the decision tree is a high variance model which is affected by two main parameters (max depth and min sample leaf). after testing we notice that increasing the max deapth led to high variance (overfitting) and low bias. On the other hand, increasing the min sample leaf led to law variance (underfitting) and high bias.\n",
        "\n"
      ],
      "metadata": {
        "id": "mXziOuKf8O-Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "task 4\n",
        "when False Negative are costly, we need to adjust the model to be more sensitive to positive. therefore we need to minimize the classification threshold\n",
        "after we lower the threshold thats mean more samples are labled as positive and FN decreased.the goal is to find the lowest threshold that minimizes FN without too many FP\n",
        "threshould should be between 0 and 1 (0< threshold > 1) otherwise it looks not logically.                                         "
      ],
      "metadata": {
        "id": "tgbOED3H9NsE"
      }
    }
  ]
}