{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ghada-ali123/AI_exercise/blob/main/Solutin_of_week_08_nlp_I.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3K4CYwe_JQkP"
      },
      "source": [
        "# Chapter 4: Shakespeare NLP Exercises: From Classical NLP to Neural Language Models\n",
        "\n",
        "**This exercise has 20 smaller tasks, for a total of 18 points, and you will have two weeks to complete it instead of one. Don't forget to submit your solutions to GitHub!**\n",
        "\n",
        "Welcome to this comprehensive hands-on workshop on Natural Language Processing! You will journey from classical text processing techniques to building and comparing neural language models using Shakespeare's complete works.\n",
        "\n",
        "## What You Will Learn\n",
        "\n",
        "- **Stage 0**: Corpus processing, tokenization, and word embeddings\n",
        "- **Stage 1**: Character-level RNN language models\n",
        "- **Stage 2**: Word-level RNN language models with theatrical chat interfaces\n",
        "- **Stage 3**: LSTM language models and architecture comparison\n",
        "\n",
        "## Instructions for Students\n",
        "\n",
        "Throughout this notebook, you will find code sections marked with:\n",
        "```python\n",
        "# START STUDENT CODE\n",
        "...\n",
        "# END STUDENT CODE\n",
        "```\n",
        "\n",
        "Any and all required tasks can and should be completed by writing code into these sections.\n",
        "\n",
        "XXXXX lösungen rauscutten, hinweise aus non-solution notebook wieder einpflegen\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kG1R8GtiJQkQ"
      },
      "source": [
        "## Setup and Dependencies\n",
        "\n",
        "First, let's install the required packages and check our environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mrf4X7maJQkQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93a3e497-c513-4053-ade2-765c9b8fe932"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.9.0+cu126\n",
            "Using device: cuda\n",
            "GPU: Tesla T4\n",
            "Data directories created.\n"
          ]
        }
      ],
      "source": [
        "# Install required packages (uncomment if running in Colab)\n",
        "# !pip install torch numpy requests\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import requests\n",
        "from collections import Counter\n",
        "\n",
        "# Check device availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Using device: {device}\")\n",
        "if device.type == 'cuda':\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# Create data directories\n",
        "os.makedirs(\"data/works\", exist_ok=True)\n",
        "print(\"Data directories created.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "go9CEh7-JQkQ"
      },
      "source": [
        "---\n",
        "\n",
        "# Stage 0: Corpus & Classical NLP Foundations\n",
        "\n",
        "In this stage, you will work with the complete works of Shakespeare to learn fundamental text processing techniques:\n",
        "- Downloading and segmenting a large text corpus\n",
        "- Tokenization and normalization\n",
        "- Working with pretrained word embeddings (GloVe)\n",
        "\n",
        "---\n",
        "\n",
        "## Stage 0.1 – Shakespeare Corpus Download & Segmentation\n",
        "\n",
        "This exercise introduces you to working with a **real, unstructured text corpus**. You will download the complete works of William Shakespeare from Project Gutenberg and convert the raw file into a collection of **separate, clean text files**, one per work. To make sure everyone can actually start working on the exercises and gets stuck right at the start, we provide code that starts you off and does the following things for you:\n",
        "\n",
        "1. **Download the Shakespeare corpus** from Project Gutenberg\n",
        "2. **Analyze the structure** - find the Table of Contents and title markers\n",
        "3. **Segment the corpus** into separate work files\n",
        "4. **Verify your segmentation** by printing statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jNVIR58lJQkR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ea9fdf8-99d1-4f3e-c4e4-b28f25b14e10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading corpus...\n",
            "Download complete.\n",
            "Found Table of Contents around line 34\n",
            "Identified 44 works from TOC.\n",
            "\n",
            "======================================================================\n",
            "SEGMENTATION SUMMARY\n",
            "======================================================================\n",
            "Work Title                                              Chars    Lines\n",
            "----------------------------------------------------------------------\n",
            "THE SONNETS                                             98332     2777\n",
            "ALL’S WELL THAT ENDS WELL                              134624     4964\n",
            "THE TRAGEDY OF ANTONY AND CLEOPATRA                    152400     6641\n",
            "AS YOU LIKE IT                                         127042     4438\n",
            "THE COMEDY OF ERRORS                                    88333     3201\n",
            "THE TRAGEDY OF CORIOLANUS                              165954     6445\n",
            "CYMBELINE                                              161238     5885\n",
            "THE TRAGEDY OF HAMLET, PRINCE OF DENMARK               177938     6698\n",
            "THE FIRST PART OF KING HENRY THE FOURTH                141709     4807\n",
            "THE SECOND PART OF KING HENRY THE FOURTH               153547     5196\n",
            "THE LIFE OF KING HENRY THE FIFTH                       153226     4946\n",
            "THE FIRST PART OF HENRY THE SIXTH                      130854     4630\n",
            "THE SECOND PART OF KING HENRY THE SIXTH                150503     5182\n",
            "THE THIRD PART OF KING HENRY THE SIXTH                 146004     5122\n",
            "KING HENRY THE EIGHTH                                  143802     5139\n",
            "THE LIFE AND DEATH OF KING JOHN                        121399     4100\n",
            "THE TRAGEDY OF JULIUS CAESAR                           116492     4647\n",
            "THE TRAGEDY OF KING LEAR                               155335     6109\n",
            "LOVE’S LABOUR’S LOST                                   127480     5008\n",
            "THE TRAGEDY OF MACBETH                                 104455     4150\n",
            "MEASURE FOR MEASURE                                    126279     4884\n",
            "THE MERCHANT OF VENICE                                 121274     4171\n",
            "THE MERRY WIVES OF WINDSOR                             130500     4823\n",
            "A MIDSUMMER NIGHT’S DREAM                               96731     3485\n",
            "MUCH ADO ABOUT NOTHING                                 122748     4600\n",
            "THE TRAGEDY OF OTHELLO, THE MOOR OF VENICE             154219     6282\n",
            "PERICLES, PRINCE OF TYRE                               111406     4193\n",
            "KING RICHARD THE SECOND                                130059     4287\n",
            "KING RICHARD THE THIRD                                 176462     6475\n",
            "THE TRAGEDY OF ROMEO AND JULIET                        142446     5267\n",
            "THE TAMING OF THE SHREW                                123751     4868\n",
            "THE TEMPEST                                             98748     3836\n",
            "THE LIFE OF TIMON OF ATHENS                            111694     4421\n",
            "THE TRAGEDY OF TITUS ANDRONICUS                        120509     4125\n",
            "TROILUS AND CRESSIDA                                   157375     6204\n",
            "TWELFTH NIGHT; OR, WHAT YOU WILL                       115458     4496\n",
            "THE TWO GENTLEMEN OF VERONA                            101929     4251\n",
            "THE TWO NOBLE KINSMEN                                  142421     5511\n",
            "THE WINTER’S TALE                                      144023     5015\n",
            "A LOVER’S COMPLAINT                                     14364      383\n",
            "THE PASSIONATE PILGRIM                                  17060      582\n",
            "THE PHOENIX AND THE TURTLE                               2075       94\n",
            "THE RAPE OF LUCRECE                                     86811     2187\n",
            "VENUS AND ADONIS                                        77408     1787\n",
            "\n",
            "Total works extracted: 44\n"
          ]
        }
      ],
      "source": [
        "RAW_FILE = 'data/pg100.txt'\n",
        "WORKS_DIR = 'data/works'\n",
        "\n",
        "def download_corpus():\n",
        "    \"\"\"Download the Shakespeare corpus from Project Gutenberg if not present.\"\"\"\n",
        "\n",
        "    if not os.path.exists(RAW_FILE):\n",
        "        print(f\"Downloading corpus...\")\n",
        "        url = \"https://www.gutenberg.org/cache/epub/100/pg100.txt\"\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()\n",
        "            with open(RAW_FILE, 'wb') as f:\n",
        "                f.write(response.content)\n",
        "            print(\"Download complete.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error downloading file: {e}\")\n",
        "            return False\n",
        "    else:\n",
        "        print(f\"Found {RAW_FILE}, skipping download.\")\n",
        "    return True\n",
        "\n",
        "\n",
        "def segment_corpus():\n",
        "    \"\"\"Segment the corpus into individual works.\"\"\"\n",
        "    # Read the file\n",
        "    with open(RAW_FILE, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    # Find Table of Contents\n",
        "    toc_start_idx = -1\n",
        "    for i, line in enumerate(lines):\n",
        "        if \"Contents\" in line and len(line.strip()) < 20:\n",
        "            if i < 200:  # TOC should be in first 200 lines\n",
        "                toc_start_idx = i\n",
        "                break\n",
        "\n",
        "    if toc_start_idx == -1:\n",
        "        print(\"Could not find Table of Contents.\")\n",
        "        return []\n",
        "\n",
        "    print(f\"Found Table of Contents around line {toc_start_idx + 1}\")\n",
        "\n",
        "    # Extract titles from TOC\n",
        "    potential_titles = []\n",
        "    first_candidate = None\n",
        "    toc_end_idx = -1\n",
        "\n",
        "    for i in range(toc_start_idx + 1, len(lines)):\n",
        "        stripped = lines[i].strip()\n",
        "        if not stripped:\n",
        "            continue\n",
        "\n",
        "        if first_candidate is None:\n",
        "            first_candidate = stripped\n",
        "            potential_titles.append(stripped)\n",
        "            continue\n",
        "\n",
        "        # Check if this line matches the first candidate (start of first work)\n",
        "        if stripped == first_candidate:\n",
        "            toc_end_idx = i\n",
        "            break\n",
        "\n",
        "        potential_titles.append(stripped)\n",
        "\n",
        "        if i > 3000:  # Safety break\n",
        "            print(\"Warning: TOC parsing went too far.\")\n",
        "            break\n",
        "\n",
        "    if toc_end_idx == -1:\n",
        "        print(\"Could not determine end of TOC.\")\n",
        "        return []\n",
        "\n",
        "    works_titles = potential_titles\n",
        "    print(f\"Identified {len(works_titles)} works from TOC.\")\n",
        "\n",
        "    # Find start positions of each work\n",
        "    work_starts = {}\n",
        "    current_search_idx = toc_end_idx\n",
        "    work_starts[works_titles[0]] = current_search_idx\n",
        "\n",
        "    for k in range(1, len(works_titles)):\n",
        "        title = works_titles[k]\n",
        "        found = False\n",
        "        for j in range(current_search_idx + 1, len(lines)):\n",
        "            if lines[j].strip() == title:\n",
        "                work_starts[title] = j\n",
        "                current_search_idx = j\n",
        "                found = True\n",
        "                break\n",
        "        if not found:\n",
        "            print(f\"Warning: Could not find start of '{title}'\")\n",
        "\n",
        "    # Write to files\n",
        "    os.makedirs(WORKS_DIR, exist_ok=True)\n",
        "    sorted_works = sorted(work_starts.items(), key=lambda x: x[1])\n",
        "\n",
        "    extracted_works = []\n",
        "    for i in range(len(sorted_works)):\n",
        "        title, start_line = sorted_works[i]\n",
        "\n",
        "        if i < len(sorted_works) - 1:\n",
        "            end_line = sorted_works[i + 1][1]\n",
        "        else:\n",
        "            end_line = len(lines)\n",
        "\n",
        "        content_lines = lines[start_line:end_line]\n",
        "        text_content = \"\".join(content_lines)\n",
        "\n",
        "        # Clean title for filename\n",
        "        safe_title = re.sub(r'[^\\w\\s-]', '', title).strip().lower()\n",
        "        safe_title = re.sub(r'[\\s-]+', '_', safe_title)\n",
        "        filename = f\"{safe_title}.txt\"\n",
        "\n",
        "        out_path = os.path.join(WORKS_DIR, filename)\n",
        "        with open(out_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(text_content)\n",
        "\n",
        "        extracted_works.append((title, filename, len(text_content), len(content_lines)))\n",
        "\n",
        "    return extracted_works\n",
        "    # Task 0.2: END STUDENT CODE\n",
        "\n",
        "# Run the corpus download and segmentation\n",
        "if download_corpus():\n",
        "    works = segment_corpus()\n",
        "\n",
        "    if works:\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"SEGMENTATION SUMMARY\")\n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\"{'Work Title':<50} {'Chars':>10} {'Lines':>8}\")\n",
        "        print(\"-\" * 70)\n",
        "        for title, filename, chars, lines_count in works:\n",
        "            print(f\"{title[:48]:<50} {chars:>10} {lines_count:>8}\")\n",
        "        print(f\"\\nTotal works extracted: {len(works)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlC1EEi9JQkR"
      },
      "source": [
        "---\n",
        "\n",
        "## Exercise 0.2 – Basic Tokenization and Normalization\n",
        "\n",
        "### Description\n",
        "\n",
        "In this exercise, you will build foundational text-processing utilities. You will design a simple **tokenizer** and apply basic **normalization** steps to Shakespeare's works. This mirrors the early stages of many NLP pipelines.\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "After completing this exercise, you should be able to:\n",
        "- Implement a minimal **tokenizer** for plain-text data\n",
        "- Apply common **normalization** steps such as lowercasing and punctuation handling\n",
        "- Inspect token distributions to understand corpus characteristics\n",
        "- Compute statistics for single works and the entire corpus\n",
        "\n",
        "### Tasks\n",
        "\n",
        "1. **Implement a basic tokenizer (3x0.5 points)** that:\n",
        "   - Splits text into word-like units\n",
        "   - Treats whitespace as a separator\n",
        "   - Separates punctuation into its own tokens\n",
        "   \n",
        "2. **Add normalization steps (1 point)**:\n",
        "   - Lowercase all tokens\n",
        "   - Normalize curly/smart quotes to straight quotes\n",
        "   \n",
        "3. **Inspect tokenized output** and compute statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eHbteSPuJQkR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da726aac-a060-430a-daf9-06c248c73961"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzing: the_tragedy_of_romeo_and_juliet.txt\n",
            "\n",
            "Total tokens: 33150\n",
            "Unique tokens (vocabulary size): 3783\n",
            "\n",
            "--- First 50 tokens ---\n",
            "['the', 'tragedy', 'of', 'romeo', 'and', 'juliet', 'contents', 'the', 'prologue', '.', 'act', 'i', 'scene', 'i', '.', 'a', 'public', 'place', '.', 'scene', 'ii', '.', 'a', 'street', '.', 'scene', 'iii', '.', 'room', 'in', \"capulet's\", 'house', '.', 'scene', 'iv', '.', 'a', 'street', '.', 'scene', 'v', '.', 'a', 'hall', 'in', \"capulet's\", 'house', '.', 'act', 'ii']\n",
            "\n",
            "--- Top 10 most frequent tokens ---\n",
            "  ',': 2704\n",
            "  '.': 2600\n",
            "  'and': 736\n",
            "  'the': 688\n",
            "  'i': 583\n",
            "  'to': 541\n",
            "  'a': 488\n",
            "  'of': 395\n",
            "  '?': 369\n",
            "  'my': 356\n",
            "\n",
            "--- Sample rare tokens ---\n",
            "  'figure': 1\n",
            "  'sacrifices': 1\n",
            "  'glooming': 1\n",
            "  'pardon'd': 1\n",
            "  'punished': 1\n"
          ]
        }
      ],
      "source": [
        "# Exercise 0.2: Basic Tokenization and Normalization\n",
        "\n",
        "\n",
        "def tokenize(text: str) -> list[str]:\n",
        "    \"\"\"\n",
        "    Tokenizes the input text into a list of strings.\n",
        "\n",
        "    Design Decisions:\n",
        "    1. Lowercase: Applied to reduce vocabulary size.\n",
        "    2. Punctuation: Separated from words into their own tokens.\n",
        "    3. Contractions: Kept together using regex (e.g., \"don't\" stays as one token).\n",
        "\n",
        "    Args:\n",
        "        text: Input text string\n",
        "\n",
        "    Returns:\n",
        "        List of token strings\n",
        "    \"\"\"\n",
        "    # Task 0.3: START STUDENT CODE\n",
        "\n",
        "    # HINT:\n",
        "    # 1. Lowercase the text\n",
        "    # 2. Normalize smart quotes ('', \"\") to straight quotes (' ', \"\")\n",
        "    # 3. Replace multiple whitespace chars (newlines, tabs) with single space\n",
        "    # 4. Use re.findall() with a regex pattern to extract tokens:\n",
        "    #    - Words with optional contractions: \\w+(?:'\\w+)?\n",
        "    #    - Single punctuation: [^\\w\\s]\n",
        "    # 5. Return the list of tokens\n",
        "\n",
        "    # 1. Lowercase the text\n",
        "    text = text.lower()\n",
        "\n",
        "    # 2. Normalize smart quotes to straight quotes\n",
        "    text = text.replace(\"’\", \"'\").replace(\"‘\", \"'\")\n",
        "    text = text.replace(\"“\", '\"').replace(\"”\", '\"')\n",
        "\n",
        "    # 3. Normalize whitespace (newlines, tabs → single space)\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "\n",
        "    # 4. Extract tokens using regex\n",
        "    tokens = re.findall(\n",
        "        r\"\\w+(?:'\\w+)?|[^\\w\\s]\",\n",
        "        text\n",
        "    )\n",
        "\n",
        "    # 5. Return tokens\n",
        "    return tokens\n",
        "\n",
        "    # Task 0.3: END STUDENT CODE\n",
        "\n",
        "# Test the tokenizer on a sample work\n",
        "sample_work = 'the_tragedy_of_romeo_and_juliet.txt'\n",
        "sample_path = os.path.join(WORKS_DIR, sample_work)\n",
        "\n",
        "if os.path.exists(sample_path):\n",
        "    with open(sample_path, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "\n",
        "    tokens = tokenize(text)\n",
        "    vocab = Counter(tokens)\n",
        "\n",
        "    print(f\"Analyzing: {sample_work}\")\n",
        "    print(f\"\\nTotal tokens: {len(tokens)}\")\n",
        "    print(f\"Unique tokens (vocabulary size): {len(vocab)}\")\n",
        "\n",
        "    print(\"\\n--- First 50 tokens ---\")\n",
        "    print(tokens[:50])\n",
        "\n",
        "    print(\"\\n--- Top 10 most frequent tokens ---\")\n",
        "    for token, count in vocab.most_common(10):\n",
        "        print(f\"  '{token}': {count}\")\n",
        "\n",
        "    print(\"\\n--- Sample rare tokens ---\")\n",
        "    for token, count in vocab.most_common()[-5:]:\n",
        "        print(f\"  '{token}': {count}\")\n",
        "else:\n",
        "    print(f\"Sample work not found at {sample_path}. Run Exercise 0.1 first.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aPdxk4hxJQkR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb676a8c-c533-45f0-ac02-61dc696a6a24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "CORPUS-WIDE STATISTICS\n",
            "======================================================================\n",
            "\n",
            "Found 44 works in the corpus.\n",
            "\n",
            "Work                                               Total Tokens  Unique Tokens\n",
            "----------------------------------------------------------------------------\n",
            "the_tragedy_of_hamlet_prince_of_denmark.txt               40789           4828\n",
            "king_richard_the_third.txt                                39454           4125\n",
            "the_tragedy_of_coriolanus.txt                             37220           4134\n",
            "cymbeline.txt                                             36677           4335\n",
            "the_tragedy_of_othello_the_moor_of_venice.txt             35978           3879\n",
            "the_tragedy_of_king_lear.txt                              35786           4271\n",
            "troilus_and_cressida.txt                                  35646           4323\n",
            "the_second_part_of_king_henry_the_fourth.txt              34849           4176\n",
            "the_tragedy_of_antony_and_cleopatra.txt                   34531           4027\n",
            "the_life_of_king_henry_the_fifth.txt                      34270           4628\n",
            "... (showing top 10 by token count)\n",
            "\n",
            "======================================================================\n",
            "AGGREGATED STATISTICS\n",
            "======================================================================\n",
            "\n",
            "Total tokens across all works: 1,221,061\n",
            "Unique tokens (vocabulary size): 28,383\n",
            "Average tokens per work: 27,751\n",
            "\n",
            "--- Top 20 Most Common Tokens in Corpus ---\n",
            "                ,:    94713 occurrences\n",
            "                .:    91379 occurrences\n",
            "              the:    30467 occurrences\n",
            "              and:    28552 occurrences\n",
            "                i:    22197 occurrences\n",
            "               to:    20745 occurrences\n",
            "               of:    18838 occurrences\n",
            "                a:    16492 occurrences\n",
            "              you:    14470 occurrences\n",
            "               my:    13201 occurrences\n",
            "                ;:    12919 occurrences\n",
            "               in:    12492 occurrences\n",
            "             that:    11840 occurrences\n",
            "                ?:    11396 occurrences\n",
            "               is:     9762 occurrences\n",
            "              not:     9095 occurrences\n",
            "             with:     8556 occurrences\n",
            "               me:     8277 occurrences\n",
            "              for:     8226 occurrences\n",
            "               it:     8218 occurrences\n",
            "\n",
            "--- Token Coverage Statistics ---\n",
            "  Top   10 tokens cover  28.7% of all tokens\n",
            "  Top   50 tokens cover  49.8% of all tokens\n",
            "  Top  100 tokens cover  59.5% of all tokens\n",
            "  Top  500 tokens cover  76.3% of all tokens\n",
            "  Top 1000 tokens cover  82.7% of all tokens\n"
          ]
        }
      ],
      "source": [
        "# Exercise 0.2 (continued): Corpus-wide Statistics\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"CORPUS-WIDE STATISTICS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Collect statistics from all works\n",
        "work_files = [f for f in os.listdir(WORKS_DIR) if f.endswith('.txt')]\n",
        "print(f\"\\nFound {len(work_files)} works in the corpus.\\n\")\n",
        "\n",
        "corpus_tokens = []\n",
        "corpus_vocab = Counter()\n",
        "work_stats = []\n",
        "\n",
        "for work_file in sorted(work_files):\n",
        "    work_path = os.path.join(WORKS_DIR, work_file)\n",
        "    with open(work_path, 'r', encoding='utf-8') as f:\n",
        "        work_text = f.read()\n",
        "\n",
        "    work_tokens = tokenize(work_text)\n",
        "    work_vocab = Counter(work_tokens)\n",
        "\n",
        "    corpus_tokens.extend(work_tokens)\n",
        "    corpus_vocab.update(work_vocab)\n",
        "\n",
        "    work_stats.append({\n",
        "        'name': work_file,\n",
        "        'total_tokens': len(work_tokens),\n",
        "        'unique_tokens': len(work_vocab)\n",
        "    })\n",
        "\n",
        "# Print per-work summary\n",
        "print(f\"{'Work':<50} {'Total Tokens':>12} {'Unique Tokens':>14}\")\n",
        "print(\"-\" * 76)\n",
        "for stat in sorted(work_stats, key=lambda x: x['total_tokens'], reverse=True)[:10]:\n",
        "    print(f\"{stat['name']:<50} {stat['total_tokens']:>12} {stat['unique_tokens']:>14}\")\n",
        "print(\"... (showing top 10 by token count)\")\n",
        "\n",
        "# Print corpus-wide statistics\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"AGGREGATED STATISTICS\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nTotal tokens across all works: {len(corpus_tokens):,}\")\n",
        "print(f\"Unique tokens (vocabulary size): {len(corpus_vocab):,}\")\n",
        "print(f\"Average tokens per work: {len(corpus_tokens) / len(work_files):,.0f}\")\n",
        "\n",
        "print(\"\\n--- Top 20 Most Common Tokens in Corpus ---\")\n",
        "for token, count in corpus_vocab.most_common(20):\n",
        "    print(f\"  {token:>15}: {count:>8} occurrences\")\n",
        "\n",
        "print(\"\\n--- Token Coverage Statistics ---\")\n",
        "total_tokens = len(corpus_tokens)\n",
        "for n in [10, 50, 100, 500, 1000]:\n",
        "    top_n_count = sum(count for _, count in corpus_vocab.most_common(n))\n",
        "    coverage = (top_n_count / total_tokens) * 100\n",
        "    print(f\"  Top {n:>4} tokens cover {coverage:>5.1f}% of all tokens\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_m3DOuAtJQkR"
      },
      "source": [
        "---\n",
        "\n",
        "## Exercise 0.3 – Working With Pretrained Word Embeddings\n",
        "\n",
        "### Description\n",
        "\n",
        "In this exercise, you will download a widely used pretrained word embedding model (GloVe) and explore semantic relationships between words by computing cosine similarities manually.\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "After completing this exercise, you should be able to:\n",
        "- Load a pretrained embedding model into Python\n",
        "- Implement cosine similarity manually using tensor operations\n",
        "- Compute and analyze similarities between selected word pairs\n",
        "- Observe how semantic and syntactic relationships are reflected in vector space\n",
        "- Perform analogy operations (e.g., king - man + woman ≈ queen)\n",
        "\n",
        "### Tasks\n",
        "\n",
        "1. Download GloVe embeddings (100-dimensional version)\n",
        "2. Load embeddings into a dictionary\n",
        "3. **Implement cosine similarity** manually **(0.5 points)**\n",
        "4. Explore semantic relationships** between word pairs - this requires **building a find_nearest function** that finds the nearest neighbor from a set of input vectors and a target vector. **(1 point)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Lx3XhJnhJQkR"
      },
      "outputs": [],
      "source": [
        "# Exercise 0.3: Working With Pretrained Word Embeddings\n",
        "\n",
        "GLOVE_PATH = 'data/glove.6B.100d.txt'\n",
        "\n",
        "\n",
        "def load_glove(path: str) -> dict:\n",
        "    \"\"\"\n",
        "    Load GloVe embeddings from a file.\n",
        "\n",
        "    Args:\n",
        "        path: Path to the GloVe file\n",
        "\n",
        "    Returns:\n",
        "        Dictionary mapping words to torch tensors\n",
        "    \"\"\"\n",
        "    print(f\"Loading GloVe from {path}...\")\n",
        "    embeddings = {}\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            try:\n",
        "                vector = np.array(values[1:], dtype='float32')\n",
        "                if len(vector) == 100:  # Ensure correct dimension\n",
        "                    embeddings[word] = torch.tensor(vector)\n",
        "            except ValueError:\n",
        "                continue\n",
        "    print(f\"Loaded {len(embeddings)} words.\")\n",
        "    return embeddings\n",
        "\n",
        "\n",
        "def cosine_similarity(vec1: torch.Tensor, vec2: torch.Tensor) -> float:\n",
        "    \"\"\"\n",
        "    Compute cosine similarity between two vectors manually.\n",
        "\n",
        "    Formula: cos(θ) = (A · B) / (|A| * |B|)\n",
        "\n",
        "    Args:\n",
        "        vec1: First vector\n",
        "        vec2: Second vector\n",
        "\n",
        "    Returns:\n",
        "        Cosine similarity as a float\n",
        "    \"\"\"\n",
        "    # Task 1.1: START STUDENT CODE\n",
        "\n",
        "    # HINT: Implement the cosine similarity formula: cos(θ) = (A · B) / (|A| * |B|)\n",
        "    # 1. Compute dot product: torch.dot(vec1, vec2)\n",
        "    # 2. Compute norms: torch.norm()\n",
        "    # 3. Handle zero norms (avoid division by zero)\n",
        "    # 4. Return the result as a Python float using .item()\n",
        "\n",
        "    # 1. Compute dot product\n",
        "    dot_product = torch.dot(vec1, vec2)\n",
        "\n",
        "    # 2. Compute vector norms\n",
        "    norm_vec1 = torch.norm(vec1)\n",
        "    norm_vec2 = torch.norm(vec2)\n",
        "\n",
        "    # 3. Avoid division by zero\n",
        "    if norm_vec1 == 0 or norm_vec2 == 0:\n",
        "        return 0.0\n",
        "\n",
        "    # 4. Compute cosine similarity\n",
        "    cosine_sim = dot_product / (norm_vec1 * norm_vec2)\n",
        "\n",
        "    # 5. Return as Python float\n",
        "    return cosine_sim.item()\n",
        "\n",
        "    # Task 1.1: END STUDENT CODE\n",
        "\n",
        "\n",
        "def find_nearest(embeddings: dict, target_vec: torch.Tensor, n: int = 5,\n",
        "                 exclude_words: list = None) -> list:\n",
        "    \"\"\"\n",
        "    Find the n nearest neighbors to a target vector by computing\n",
        "    the dot product of each word embedding with the target vector.\n",
        "\n",
        "    Args:\n",
        "        embeddings: Dictionary of word embeddings\n",
        "        target_vec: Target vector to find neighbors for\n",
        "        n: Number of neighbors to return\n",
        "        exclude_words: Words to exclude from results\n",
        "\n",
        "    Returns:\n",
        "        List of (word, similarity) tuples\n",
        "    \"\"\"\n",
        "    # Task 1.2: START STUDENT CODE\n",
        "\n",
        "    # HINT:\n",
        "    # 1. Convert embeddings dict to lists: words and vocab_matrix (stacked tensors)\n",
        "    # 2. Compute norms for all vocabulary vectors and the target vector\n",
        "    # 3. Compute dot products between vocab_matrix and target_vec using matmul\n",
        "    # 4. Compute cosine similarities using the formula\n",
        "    # 5. Use torch.topk() to find top k highest scores\n",
        "    # 6. Filter out excluded_words and return top n results as (word, score) tuples\n",
        "\n",
        "    if exclude_words is None:\n",
        "        exclude_words = []\n",
        "\n",
        "    # 1. Convert embeddings dict to lists\n",
        "    words = []\n",
        "    vectors = []\n",
        "\n",
        "    for word, vec in embeddings.items():\n",
        "        if word not in exclude_words:\n",
        "            words.append(word)\n",
        "            vectors.append(vec)\n",
        "\n",
        "    # Stack all vectors into a single tensor (vocab_size x embedding_dim)\n",
        "    vocab_matrix = torch.stack(vectors)\n",
        "\n",
        "    # 2. Compute norms\n",
        "    vocab_norms = torch.norm(vocab_matrix, dim=1)\n",
        "    target_norm = torch.norm(target_vec)\n",
        "\n",
        "    # Avoid division by zero\n",
        "    if target_norm == 0:\n",
        "        return []\n",
        "\n",
        "    # 3. Compute dot products\n",
        "    dot_products = torch.matmul(vocab_matrix, target_vec)\n",
        "\n",
        "    # 4. Compute cosine similarities\n",
        "    cosine_similarities = dot_products / (vocab_norms * target_norm)\n",
        "\n",
        "    # 5. Get top-k highest similarities\n",
        "    top_scores, top_indices = torch.topk(cosine_similarities, k=n)\n",
        "\n",
        "    # 6. Return results\n",
        "    results = []\n",
        "    for score, idx in zip(top_scores, top_indices):\n",
        "        results.append((words[idx], score.item()))\n",
        "\n",
        "    return results\n",
        "    # Task 1.2: END STUDENT CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7uFZCV91w9Rd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e4ab852-3a4d-4fb9-8ee0-6cf673fd2bf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-16 19:55:06--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2025-12-16 19:55:06--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2025-12-16 19:55:06--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.02MB/s    in 2m 40s  \n",
            "\n",
            "2025-12-16 19:57:46 (5.14 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: data/glove.6B.50d.txt   \n",
            "  inflating: data/glove.6B.100d.txt  \n",
            "  inflating: data/glove.6B.200d.txt  \n",
            "  inflating: data/glove.6B.300d.txt  \n"
          ]
        }
      ],
      "source": [
        "# Download GLOVE - you only need to do this once.\n",
        "\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip -d data/\n",
        "\n",
        "# If this fails, you can download manually from:\n",
        "# https://nlp.stanford.edu/data/glove.6B.zip\n",
        "# and extract the zip file into the /data directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tfZM7NPVw9Re",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61d79463-e9c9-4274-bc45-182fe7baa3ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading GloVe from data/glove.6B.100d.txt...\n",
            "Loaded 400000 words.\n",
            "\n",
            "--- Cosine Similarity Between Word Pairs ---\n",
            "  king         - monarch     : 0.6978\n",
            "  love         - affection   : 0.6255\n",
            "  war          - peace       : 0.6155\n",
            "  love         - hate        : 0.5704\n",
            "  doctor       - nurse       : 0.7522\n",
            "  poison       - dagger      : 0.3359\n",
            "  romeo        - juliet      : 0.6607\n",
            "  tragedy      - comedy      : 0.3790\n",
            "\n",
            "--- Word Analogies ---\n",
            "\n",
            "  king - man + woman = ?\n",
            "    queen: 0.7834\n",
            "    monarch: 0.6934\n",
            "    throne: 0.6833\n",
            "    daughter: 0.6809\n",
            "    prince: 0.6713\n",
            "\n",
            "  paris - france + italy = ?\n",
            "    rome: 0.8084\n",
            "    milan: 0.7317\n",
            "    naples: 0.7090\n",
            "    venice: 0.7010\n",
            "    turin: 0.6970\n",
            "\n",
            "  father - man + woman = ?\n",
            "    mother: 0.9137\n",
            "    daughter: 0.8749\n",
            "    wife: 0.8636\n",
            "    husband: 0.8385\n",
            "    grandmother: 0.8167\n"
          ]
        }
      ],
      "source": [
        "# Load embeddings and explore relationships\n",
        "\n",
        "if os.path.exists(GLOVE_PATH):\n",
        "    embeddings = load_glove(GLOVE_PATH)\n",
        "\n",
        "    # Explore semantic relationships\n",
        "    pairs = [\n",
        "        (\"king\", \"monarch\"), (\"love\", \"affection\"),  # Synonyms\n",
        "        (\"war\", \"peace\"), (\"love\", \"hate\"),  # Antonyms\n",
        "        (\"doctor\", \"nurse\"), (\"poison\", \"dagger\"),  # Semantic fields\n",
        "        (\"romeo\", \"juliet\"), (\"tragedy\", \"comedy\")  # Shakespeare-related\n",
        "    ]\n",
        "\n",
        "    print(\"\\n--- Cosine Similarity Between Word Pairs ---\")\n",
        "    for w1, w2 in pairs:\n",
        "        if w1 in embeddings and w2 in embeddings:\n",
        "            sim = cosine_similarity(embeddings[w1], embeddings[w2])\n",
        "            print(f\"  {w1:12} - {w2:12}: {sim:.4f}\")\n",
        "        else:\n",
        "            missing = [w for w in [w1, w2] if w not in embeddings]\n",
        "            print(f\"  Missing: {missing}\")\n",
        "\n",
        "    # Analogies\n",
        "    print(\"\\n--- Word Analogies ---\")\n",
        "\n",
        "    def solve_analogy(pos1, neg1, pos2):\n",
        "        \"\"\"Solve: pos1 - neg1 + pos2 = ?\"\"\"\n",
        "        print(f\"\\n  {pos1} - {neg1} + {pos2} = ?\")\n",
        "        if all(w in embeddings for w in [pos1, neg1, pos2]):\n",
        "            vec = embeddings[pos1] - embeddings[neg1] + embeddings[pos2]\n",
        "            neighbors = find_nearest(embeddings, vec, n=5, exclude_words=[pos1, neg1, pos2])\n",
        "            for word, score in neighbors:\n",
        "                print(f\"    {word}: {score:.4f}\")\n",
        "        else:\n",
        "            print(\"    Word(s) not in vocabulary.\")\n",
        "\n",
        "    solve_analogy(\"king\", \"man\", \"woman\")  # Expected: queen\n",
        "    solve_analogy(\"paris\", \"france\", \"italy\")  # Expected: rome\n",
        "    solve_analogy(\"father\", \"man\", \"woman\")  # Expected: mother\n",
        "else:\n",
        "    print(f\"GloVe file not found at {GLOVE_PATH}\")\n",
        "    print(\"Please download from: https://nlp.stanford.edu/data/glove.6B.zip\")\n",
        "    print(\"Extract glove.6B.100d.txt to the data/ directory.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-kj6IzRJQkR"
      },
      "source": [
        "---\n",
        "\n",
        "# Stage 1: Character-Level Language Modeling\n",
        "\n",
        "In this stage, you will build your first **neural language model** over Shakespeare's text using a **character-level recurrent neural network (RNN)** in PyTorch. You will:\n",
        "\n",
        "- Construct a character-level dataset from one work\n",
        "- Implement and train an RNN-based language model (on GPU if available)\n",
        "- Generate text using greedy decoding and temperature sampling\n",
        "\n",
        "---\n",
        "\n",
        "## Exercise 1.1 – Character Vocabulary and Sequential Dataset\n",
        "\n",
        "### Description\n",
        "\n",
        "You will construct a **character-level representation** of a Shakespeare work and prepare a dataset for **next-character prediction**.\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "After completing this exercise, you should be able to:\n",
        "- Build a **character vocabulary** from raw text\n",
        "- Map characters to integer indices and back\n",
        "- Prepare sliding-window input–target pairs for sequence modeling\n",
        "- Wrap the data in a PyTorch `Dataset` and `DataLoader`\n",
        "\n",
        "### Task\n",
        "\n",
        "Implement a PyTorch Dataset class that implements the usual functions; \\_\\_init\\_\\_, \\_\\_len\\_\\_, and \\_\\_getitem\\_\\_. The init function needs to take the text corpus as a string and a sequence length as an integer. The getitem function takes an index integer as usual and returns a number of characters (sequence length) starting at that index in the text corpus - this is the data - and a target that has the same length, but which is offset by one to the right. For example, if the corpus is the text `Hello, I am a dog.`, then the output at sequence length 5 and index 0 would be `Hello` (data) and `ello,` (target). **(3 x 0.5 points)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0CGVClVmJQkS"
      },
      "outputs": [],
      "source": [
        "# Exercise 1.1: Character Vocabulary and Sequential Dataset\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "\n",
        "class CharDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A PyTorch Dataset for character-level language modeling.\n",
        "\n",
        "    Creates input-target pairs using a sliding window over the text.\n",
        "    Input: characters from position t to t + seq_len - 1\n",
        "    Target: characters from position t + 1 to t + seq_len\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, text: str, seq_len: int):\n",
        "        \"\"\"\n",
        "        Initialize the dataset.\n",
        "\n",
        "        Args:\n",
        "            text: The full text as a string\n",
        "            seq_len: Length of each sequence (number of characters)\n",
        "        \"\"\"\n",
        "        # Task 1.3: START STUDENT CODE\n",
        "\n",
        "        # HINT:\n",
        "        # 1. Store text and seq_len\n",
        "        # 2. Build character set: unique chars from text, sorted for consistency\n",
        "        # 3. Create bidirectional mappings: char_to_idx and idx_to_char\n",
        "        # 4. Encode entire text as tensor of indices using char_to_idx\n",
        "\n",
        "         # 1. Store text and sequence length\n",
        "        self.text = text\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        # 2. Build character vocabulary (sorted for consistency)\n",
        "        chars = sorted(list(set(text)))\n",
        "\n",
        "        # 3. Create mappings\n",
        "        self.char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
        "        self.idx_to_char = {i: ch for ch, i in self.char_to_idx.items()}\n",
        "\n",
        "        self.vocab_size = len(chars)\n",
        "        # 4. Encode entire text as tensor of indices\n",
        "        self.data = torch.tensor(\n",
        "            [self.char_to_idx[ch] for ch in text],\n",
        "            dtype=torch.long\n",
        "        )\n",
        "\n",
        "        # Task 1.3: END STUDENT CODE\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of samples we can create from the text.\"\"\"\n",
        "        # Task 1.4: START STUDENT CODE\n",
        "\n",
        "        # HINT: Each sample needs seq_len + 1 characters (seq_len for input, last one for target)\n",
        "        # So max samples = len(self.data) - seq_len\n",
        "\n",
        "        return len(self.data) - self.seq_len\n",
        "\n",
        "        # Task 1.4: END STUDENT CODE\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Get a single sample.\n",
        "\n",
        "        Returns:\n",
        "            input_seq: tensor of shape [seq_len]\n",
        "            target_seq: tensor of shape [seq_len]\n",
        "        \"\"\"\n",
        "        # Task 1.5: START STUDENT CODE\n",
        "\n",
        "        # HINT: Implement sliding window for next-character prediction\n",
        "        # 1. Extract seq_len + 1 characters starting at idx\n",
        "        # 2. Split into input (first seq_len) and target (last seq_len, shifted by 1)\n",
        "        # 3. Return both as tensors\n",
        "\n",
        "         # 1. Extract window of length seq_len + 1\n",
        "        chunk = self.data[idx: idx + self.seq_len + 1]\n",
        "\n",
        "        # 2. Split into input and target\n",
        "        input_seq = chunk[:-1]\n",
        "        target_seq = chunk[1:]\n",
        "\n",
        "        # 3. Return both\n",
        "        return input_seq, target_seq\n",
        "\n",
        "        # Task 1.5: END STUDENT CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Jqkb0WBSw9Rf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7eb7ebaf-3952-4af3-cf6a-b44759865411"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Exercise 1.1: Character Dataset ---\n",
            "Loaded text length: 142446 characters\n",
            "Vocabulary Size: 70\n",
            "Characters: '\\n !&,-.:;?ABCDEFGHIJKLMNOPQRSTUVWYZ[]_abcdefghijklmnopqrstuvwxyzæ—‘’“”'\n",
            "Number of samples: 142346\n",
            "\n",
            "Batch Input Shape: torch.Size([64, 100])\n",
            "Batch Target Shape: torch.Size([64, 100])\n",
            "\n",
            "Sample Input (first 50 chars): ' me, let them find me here.\\nMy life were better en'\n",
            "Sample Target (first 50 chars): 'me, let them find me here.\\nMy life were better end'\n",
            "\n",
            "--- Design Notes ---\n",
            "seq_len=100: Captures sufficient context (approx 1-2 lines of verse)\n",
            "batch_size=64: Good balance between efficiency and memory usage\n"
          ]
        }
      ],
      "source": [
        "# Test the CharDataset\n",
        "print(\"--- Exercise 1.1: Character Dataset ---\")\n",
        "\n",
        "# Load Romeo and Juliet (or another work)\n",
        "work_path = os.path.join(WORKS_DIR, 'the_tragedy_of_romeo_and_juliet.txt')\n",
        "if not os.path.exists(work_path):\n",
        "    print(\"Work file not found. Please run Exercise 0.1 first.\")\n",
        "else:\n",
        "    with open(work_path, 'r', encoding='utf-8') as f:\n",
        "        char_text = f.read()\n",
        "\n",
        "    print(f\"Loaded text length: {len(char_text)} characters\")\n",
        "\n",
        "    # Create dataset\n",
        "    # seq_len=100 captures roughly 1-2 lines of verse - good context for learning structure\n",
        "    CHAR_SEQ_LEN = 100\n",
        "    CHAR_BATCH_SIZE = 64\n",
        "\n",
        "    char_dataset = CharDataset(char_text, CHAR_SEQ_LEN)\n",
        "    print(f\"Vocabulary Size: {char_dataset.vocab_size}\")\n",
        "    print(f\"Characters: {repr(''.join(char_dataset.char_to_idx.keys()))}\")\n",
        "    print(f\"Number of samples: {len(char_dataset)}\")\n",
        "\n",
        "    # Create DataLoader\n",
        "    char_dataloader = DataLoader(char_dataset, batch_size=CHAR_BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    # Verify with one batch\n",
        "    inputs, targets = next(iter(char_dataloader))\n",
        "    print(f\"\\nBatch Input Shape: {inputs.shape}\")\n",
        "    print(f\"Batch Target Shape: {targets.shape}\")\n",
        "\n",
        "    # Show a sample\n",
        "    sample_input = \"\".join([char_dataset.idx_to_char[i.item()] for i in inputs[0]])\n",
        "    sample_target = \"\".join([char_dataset.idx_to_char[i.item()] for i in targets[0]])\n",
        "    print(f\"\\nSample Input (first 50 chars): {repr(sample_input[:50])}\")\n",
        "    print(f\"Sample Target (first 50 chars): {repr(sample_target[:50])}\")\n",
        "\n",
        "    print(f\"\\n--- Design Notes ---\")\n",
        "    print(f\"seq_len={CHAR_SEQ_LEN}: Captures sufficient context (approx 1-2 lines of verse)\")\n",
        "    print(f\"batch_size={CHAR_BATCH_SIZE}: Good balance between efficiency and memory usage\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2soyETvJQkS"
      },
      "source": [
        "---\n",
        "\n",
        "## Exercise 1.2 – Character-Level RNN Language Model in PyTorch\n",
        "\n",
        "### Description\n",
        "\n",
        "You will implement and train a **character-level RNN-based language model** using the dataset from Exercise 1.1. The model will learn to predict the next character given the previous characters.\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "After completing this exercise, you should be able to:\n",
        "- Define a simple **recurrent neural network** for language modeling\n",
        "- Use an **embedding layer**, an `nn.RNN`, and a linear output layer\n",
        "- Train a neural language model with **cross-entropy loss**\n",
        "- Run training on **GPU** where available\n",
        "\n",
        "### Model Architecture\n",
        "\n",
        "```\n",
        "Input (char indices) → Embedding → RNN → Linear → Output (vocab logits)\n",
        "```\n",
        "\n",
        "### Tasks\n",
        "\n",
        "1. Set up device selection (GPU if available)\n",
        "2. Implement the `CharRNNLM` model class **(1 point)**\n",
        "3. Train the model with cross-entropy loss and Adam optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gdhl6pY8JQkS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "541da634-307d-4a01-a438-8d7c2cbc17b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Exercise 1.2: Character-Level RNN Language Model\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class CharRNNLM(nn.Module):\n",
        "    \"\"\"\n",
        "    Character-level RNN Language Model.\n",
        "\n",
        "    Architecture:\n",
        "    - Embedding layer: maps character indices to dense vectors\n",
        "    - RNN layer: processes sequences and maintains hidden state\n",
        "    - Linear layer: maps hidden states to vocabulary logits\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size: int, emb_dim: int, hidden_size: int):\n",
        "        \"\"\"\n",
        "        Initialize the model.\n",
        "\n",
        "        Args:\n",
        "            vocab_size: Number of unique characters\n",
        "            emb_dim: Dimension of character embeddings\n",
        "            hidden_size: Number of hidden units in RNN\n",
        "        \"\"\"\n",
        "        # Task 1.6: START STUDENT CODE\n",
        "\n",
        "        # HINT: Build a simple RNN-based language model\n",
        "        # 1. Call super().__init__()\n",
        "        # 2. Create embedding layer: nn.Embedding(vocab_size, emb_dim)\n",
        "        # 3. Create RNN layer: nn.RNN(emb_dim, hidden_size, batch_first=True)\n",
        "        # 4. Create linear output layer: nn.Linear(hidden_size, vocab_size)\n",
        "        # 1. Initialize parent class\n",
        "        super().__init__()\n",
        "\n",
        "        # 2. Embedding layer: character index → dense vector\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
        "\n",
        "        # 3. RNN layer\n",
        "        self.rnn = nn.RNN(\n",
        "            input_size=emb_dim,\n",
        "            hidden_size=hidden_size,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # 4. Output layer: hidden state → vocabulary logits\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        # Task 1.6: END STUDENT CODE\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch, seq_len)\n",
        "            hidden: Optional initial hidden state\n",
        "\n",
        "        Returns:\n",
        "            logits: Output logits of shape (batch, seq_len, vocab_size)\n",
        "            hidden: Final hidden state\n",
        "        \"\"\"\n",
        "        # Task 1.7: START STUDENT CODE\n",
        "\n",
        "        # HINT: Forward pass through the model:\n",
        "        # 1. Embed the input indices\n",
        "        # 2. Pass through RNN to get hidden states\n",
        "        # 3. Pass hidden states through linear layer to get logits\n",
        "        # 4. Return logits and final hidden state\n",
        "\n",
        "        # 1. Convert character indices to embeddings\n",
        "        x = self.embedding(x)  # (batch, seq_len, emb_dim)\n",
        "\n",
        "        # 2. Pass embeddings through RNN\n",
        "        output, hidden = self.rnn(x, hidden)\n",
        "        # output: (batch, seq_len, hidden_size)\n",
        "\n",
        "        # 3. Map hidden states to vocabulary scores\n",
        "        logits = self.fc(output)\n",
        "        # logits: (batch, seq_len, vocab_size)\n",
        "\n",
        "        # 4. Return logits and hidden state\n",
        "        return logits, hidden\n",
        "\n",
        "\n",
        "        # Task 1.7: END STUDENT CODE\n",
        "\n",
        "# Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "T903E9MSJQkS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "674aa648-5dcc-484e-fe6d-babd1518b8e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Exercise 1.2: Training Character RNN ---\n",
            "Model created with 104,902 parameters\n",
            "\n",
            "--- Training Notes ---\n",
            "The loss should decrease over epochs, indicating the model is learning.\n",
            "If loss doesn't decrease, try: lower learning rate, more epochs, or larger model.\n",
            "Epoch 1/5, Loss: 1.3362\n",
            "Epoch 2/5, Loss: 0.9750\n",
            "Epoch 3/5, Loss: 0.9167\n",
            "Epoch 4/5, Loss: 0.8901\n",
            "Epoch 5/5, Loss: 0.8755\n",
            "\n",
            "Model saved to char_rnn_model.pth\n"
          ]
        }
      ],
      "source": [
        "# Exercise 1.2 (continued): Training the Character RNN\n",
        "\n",
        "print(\"--- Exercise 1.2: Training Character RNN ---\")\n",
        "\n",
        "# Model hyperparameters\n",
        "CHAR_EMB_DIM = 64\n",
        "CHAR_HIDDEN_SIZE = 256\n",
        "CHAR_EPOCHS = 5\n",
        "CHAR_LR = 0.002\n",
        "\n",
        "# Create model\n",
        "char_model = CharRNNLM(\n",
        "    vocab_size=char_dataset.vocab_size,\n",
        "    emb_dim=CHAR_EMB_DIM,\n",
        "    hidden_size=CHAR_HIDDEN_SIZE\n",
        ").to(device)\n",
        "\n",
        "print(f\"Model created with {sum(p.numel() for p in char_model.parameters()):,} parameters\")\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(char_model.parameters(), lr=CHAR_LR)\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n--- Training Notes ---\")\n",
        "print(\"The loss should decrease over epochs, indicating the model is learning.\")\n",
        "print(\"If loss doesn't decrease, try: lower learning rate, more epochs, or larger model.\")\n",
        "\n",
        "# Training loop\n",
        "char_model.train()\n",
        "for epoch in range(CHAR_EPOCHS):\n",
        "    total_loss = 0\n",
        "    for i, (inputs, targets) in enumerate(char_dataloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass (hidden state starts as None/zeros)\n",
        "        logits, _ = char_model(inputs)\n",
        "\n",
        "        # Reshape for loss: (batch * seq_len, vocab_size) vs (batch * seq_len)\n",
        "        loss = criterion(logits.view(-1, char_dataset.vocab_size), targets.view(-1))\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(char_model.parameters(), 5)  # Gradient clipping for stability\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(char_dataloader)\n",
        "    print(f\"Epoch {epoch+1}/{CHAR_EPOCHS}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Save model checkpoint\n",
        "char_model_path = \"char_rnn_model.pth\"\n",
        "torch.save({\n",
        "    'model_state_dict': char_model.state_dict(),\n",
        "    'vocab_size': char_dataset.vocab_size,\n",
        "    'emb_dim': CHAR_EMB_DIM,\n",
        "    'hidden_size': CHAR_HIDDEN_SIZE,\n",
        "    'char_to_idx': char_dataset.char_to_idx,\n",
        "    'idx_to_char': char_dataset.idx_to_char,\n",
        "}, char_model_path)\n",
        "print(f\"\\nModel saved to {char_model_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAhqSmaVJQkS"
      },
      "source": [
        "---\n",
        "\n",
        "## Exercise 1.3 – Text Generation and Temperature Sampling\n",
        "\n",
        "### Description\n",
        "\n",
        "You will implement text generation functions for your character-level language model and experiment with different **sampling strategies**, including **greedy decoding** and **temperature-scaled sampling**.\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "After completing this exercise, you should be able to:\n",
        "- Generate text autoregressively from a trained character-level language model\n",
        "- Implement **greedy decoding** and observe its limitations\n",
        "- Implement **temperature-based sampling** from a categorical distribution\n",
        "- Qualitatively compare generated outputs at different temperatures\n",
        "\n",
        "### Temperature Explained\n",
        "\n",
        "- **Temperature = 0 (or very low)**: Greedy - always picks most likely character. Repetitive but \"safe\".\n",
        "- **Temperature = 1.0**: Standard sampling from the learned distribution.\n",
        "- **Temperature > 1.0**: More random/creative, but may produce nonsense.\n",
        "- **Temperature < 1.0**: More focused/conservative, less variety.\n",
        "\n",
        "### Tasks\n",
        "\n",
        "1. Implement a `generate_greedy` function - This function should generate a string of predefined length, given a model and a start_text string. **(1 point)**\n",
        "2. Implement a `generate_with_temperature` function - This function should generate a string of predefined length, given a model and a start_text string, but the generation of each new character is up to chance. The probabilities that a possible new character is selected is based on the temperature-scaled logits (our normal outputs times the temperature factor). **(1 point)**\n",
        "3. Compare outputs at different temperatures. **(0.5 points)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "9zTu0R_jJQkS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c6c3230-6bad-4c45-d03c-c1d1813123ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Exercise 1.3: Text Generation ---\n",
            "\n",
            "============================================================\n",
            "Prompt: 'ROMEO.'\n",
            "============================================================\n",
            "\n",
            "--- Greedy Decoding ---\n",
            "ROMEO.\n",
            "Thousenge\n",
            "Mercy. Juliet. Scanmon be a simple in the mother?\n",
            "\n",
            "NURSE.\n",
            "Well, sir, I will show the winds thy bridegroom is not the trough and with here,\n",
            "And then the day is love shows the tomb;\n",
            "And but t\n",
            "\n",
            "--- Temperature 0.5 ---\n",
            "ROMEO.\n",
            "O will; in this fair corse uncheer’s words,\n",
            "That will dencefortune!\n",
            "\n",
            "PETER.\n",
            "I speak no joints to be gone,\n",
            "And one of the maids of death, no son and bring eyes,\n",
            "Which with sweet’s end.\n",
            "O, in this fair\n",
            "\n",
            "--- Temperature 1.0 ---\n",
            "ROMEO.\n",
            "I may be watch thee sprive.\n",
            "This from my south.\n",
            "\n",
            "NURSE.\n",
            "Anon, neet, I’ll ney fool. Then he do not not, Juliet is this ness, wife that word’s death\n",
            "Is love, I say,\n",
            "A damn?\n",
            "Herch, and I’ll all.\n",
            "\n",
            "MERCUT\n",
            "\n",
            "--- Temperature 2.0 ---\n",
            "ROMEO.” Cymay tage\n",
            "Ferman knocy, hisees clitts.\n",
            "Haps was enten whimorriot,\n",
            "I’p shuth, I, in breats’s, my—man he beg’s, and youns. Sherefore po kid!\n",
            "\n",
            "SAMPSON.\n",
            "Festrehora, twoen O weavoub!\n",
            "CpPortiang: Here’s \n",
            "\n",
            "============================================================\n",
            "Prompt: 'The '\n",
            "============================================================\n",
            "\n",
            "--- Greedy Decoding ---\n",
            "The in my father that with his sight for a fearful brawls, but should not to be satisfied! O, wife to his father’s.\n",
            "\n",
            "MERCUTIO.\n",
            "Thou had be to his father’s.\n",
            "\n",
            "JULIET.\n",
            "What satish but for the mother, madam, \n",
            "\n",
            "--- Temperature 0.5 ---\n",
            "The in my child,\n",
            "Bearing in this chang’d with thine eyesing not for the world is not this fair and his word.\n",
            "Therefore thou art thou wilt have it you will be her bese?\n",
            "\n",
            "ROMEO.\n",
            "Out of her backs.\n",
            "\n",
            "CHORUS.\n",
            "S\n",
            "\n",
            "--- Temperature 1.0 ---\n",
            "The in life\n",
            "Faretry\n",
            "O strangem or by myself,\n",
            "But now my dear sative less away.\n",
            "\n",
            "JULIET.\n",
            "Well not mitthe wits Romeo did them fades, many other’s shalt still, a word. Despised\n",
            "And with the wonly shalt for t\n",
            "\n",
            "--- Temperature 2.0 ---\n",
            "The his Frone up,\n",
            "Two’tles, with alrnagie;\n",
            "Where’s my flowy frothery? Doth meat rich as dasc my befiebe, musicts! O woe to that, poosmorning. Wadch would I: when Wak’d,\n",
            "Serle whoe, thoughf,\n",
            "cauns\n",
            "As sweet\n"
          ]
        }
      ],
      "source": [
        "# Exercise 1.3: Text Generation and Temperature Sampling\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def generate_greedy(model, start_text: str, length: int, char_to_idx: dict,\n",
        "                    idx_to_char: dict, device='cpu') -> str:\n",
        "    \"\"\"\n",
        "    Generate text using greedy decoding (always pick most likely next character).\n",
        "\n",
        "    Args:\n",
        "        model: Trained CharRNNLM model\n",
        "        start_text: Initial prompt text\n",
        "        length: Number of characters to generate\n",
        "        char_to_idx: Character to index mapping\n",
        "        idx_to_char: Index to character mapping\n",
        "        device: Device to run on\n",
        "\n",
        "    Returns:\n",
        "        Generated text string (including prompt)\n",
        "    \"\"\"\n",
        "    # Task 1.8: START STUDENT CODE\n",
        "\n",
        "    # HINT: Implement greedy decoding for text generation\n",
        "    # 1. Set model to eval mode and move to device\n",
        "    # 2. Encode the start_text as character indices (handle unknown with fallback)\n",
        "    # 3. Process prompt through model to initialize hidden state\n",
        "    # 4. Loop for 'length' iterations:\n",
        "    #    - Pick character with highest probability (torch.argmax)\n",
        "    #    - Append to generated_text\n",
        "    #    - Feed single character to model for next step\n",
        "    # 5. Return generated text\n",
        "\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    # Encode start text\n",
        "    input_indices = [\n",
        "        char_to_idx.get(ch, 0) for ch in start_text\n",
        "    ]\n",
        "    input_tensor = torch.tensor(input_indices, dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "    generated_text = start_text\n",
        "\n",
        "    # Initialize hidden state by running the prompt through the model\n",
        "    with torch.no_grad():\n",
        "        _, hidden = model(input_tensor)\n",
        "\n",
        "        current_char = input_tensor[:, -1:]\n",
        "\n",
        "        for _ in range(length):\n",
        "            logits, hidden = model(current_char, hidden)\n",
        "            logits = logits.squeeze(1)\n",
        "            # Pick the most likely character\n",
        "            next_idx = torch.argmax(logits.squeeze(0), dim=-1)\n",
        "\n",
        "            next_char = idx_to_char[next_idx.item()]\n",
        "            generated_text += next_char\n",
        "\n",
        "            # Feed back into model\n",
        "            current_char = next_idx.view(1, 1)\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "    # Task 1.8: END STUDENT CODE\n",
        "\n",
        "\n",
        "def generate_with_temperature(model, start_text: str, length: int, temperature: float,\n",
        "                              char_to_idx: dict, idx_to_char: dict, device='cpu') -> str:\n",
        "    \"\"\"\n",
        "    Generate text using temperature-scaled sampling.\n",
        "\n",
        "    Args:\n",
        "        model: Trained CharRNNLM model\n",
        "        start_text: Initial prompt text\n",
        "        length: Number of characters to generate\n",
        "        temperature: Sampling temperature (higher = more random)\n",
        "        char_to_idx: Character to index mapping\n",
        "        idx_to_char: Index to character mapping\n",
        "        device: Device to run on\n",
        "\n",
        "    Returns:\n",
        "        Generated text string (including prompt)\n",
        "    \"\"\"\n",
        "    # Task 1.9: START STUDENT CODE\n",
        "\n",
        "    # HINT: Implement temperature-scaled sampling for more creative text\n",
        "    # 1. Set model to eval mode and move to device\n",
        "    # 2. Encode start_text and process through model\n",
        "    # 3. Loop for 'length' iterations:\n",
        "    #    - If temperature very small (~0): use greedy (torch.argmax)\n",
        "    #    - Otherwise: scale logits by temperature, softmax to probabilities, sample\n",
        "    #    - Use torch.multinomial() to sample from probability distribution\n",
        "    #    - Append character and feed to model\n",
        "    # 4. Return generated text\n",
        "\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    input_indices = [\n",
        "        char_to_idx.get(ch, 0) for ch in start_text\n",
        "    ]\n",
        "    input_tensor = torch.tensor(input_indices, dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "    generated_text = start_text\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _, hidden = model(input_tensor)\n",
        "        current_char = input_tensor[:, -1:]\n",
        "\n",
        "        for _ in range(length):\n",
        "            logits, hidden = model(current_char, hidden)\n",
        "            logits = logits.squeeze(1)\n",
        "\n",
        "            if temperature < 1e-5:\n",
        "                # Greedy fallback\n",
        "                next_idx = torch.argmax(logits, dim=-1)\n",
        "            else:\n",
        "                # Temperature scaling\n",
        "                scaled_logits = logits / temperature\n",
        "                probs = F.softmax(scaled_logits, dim=-1)\n",
        "                next_idx = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
        "\n",
        "            next_char = idx_to_char[next_idx.item()]\n",
        "            generated_text += next_char\n",
        "            current_char = next_idx.view(1, 1)\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "\n",
        "    # Task 1.9: END STUDENT CODE\n",
        "\n",
        "# Test generation\n",
        "print(\"--- Exercise 1.3: Text Generation ---\")\n",
        "\n",
        "prompts = [\"ROMEO.\", \"The \"]\n",
        "\n",
        "for prompt in prompts:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Prompt: '{prompt}'\")\n",
        "    print('='*60)\n",
        "\n",
        "    print(\"\\n--- Greedy Decoding ---\")\n",
        "    greedy_text = generate_greedy(\n",
        "        char_model, prompt, 200,\n",
        "        char_dataset.char_to_idx, char_dataset.idx_to_char, device\n",
        "    )\n",
        "    print(greedy_text)\n",
        "\n",
        "    for temp in [0.5, 1.0, 2.0]:\n",
        "        print(f\"\\n--- Temperature {temp} ---\")\n",
        "        temp_text = generate_with_temperature(\n",
        "            char_model, prompt, 200, temp,\n",
        "            char_dataset.char_to_idx, char_dataset.idx_to_char, device\n",
        "        )\n",
        "        print(temp_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QpE0X_HJQkS"
      },
      "source": [
        "---\n",
        "\n",
        "# Stage 2: Word-Level Language Modeling & Theatrical Chat Interface\n",
        "\n",
        "In this stage, you will move from **character-level** to **word-level** language modeling. You will:\n",
        "\n",
        "- Build a word-level vocabulary and dataset from Shakespeare's works\n",
        "- Implement and train a **word-level RNN language model**\n",
        "- Construct a simple **theatrical chat interface** that simulates dialog between characters\n",
        "- Create a **turn-aware** model with special end-of-turn tokens\n",
        "\n",
        "---\n",
        "\n",
        "## Exercise 2.1 – Word-Level Vocabulary and Sequential Dataset\n",
        "\n",
        "### Description\n",
        "\n",
        "You will construct a **word-level representation** of Shakespeare's text and prepare a dataset for **next-word prediction**.\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "After completing this exercise, you should be able to:\n",
        "- Extend a tokenizer for word-level modeling\n",
        "- Build a **word vocabulary** with frequency cutoffs\n",
        "- Handle out-of-vocabulary words with `<UNK>` token\n",
        "- Prepare sliding-window input-target pairs for next-word prediction\n",
        "\n",
        "### Task\n",
        "\n",
        "Instead of a character-based dataset, we now transition to a word-based dataset. The methods are the same as before, except we now split the text into words and limit ourselves to a vocabulary of a fixed size `vocab_size`, which should consist of the most common tokens in the corpus (hint: use a Counter). The vocabulary should also contain an `unk_token = \"<UNK>\"`, which is our stand-in for any future words we do not know (either rare tokens from this corpus or other texts).\n",
        "\n",
        "1. Build the vocabulary. **(0.5 points)**\n",
        "2. Build `<UNK>` handling. **(0.5 points)**\n",
        "3. Build the rest of the dataset. **(0.5 points)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "2ZgTNZqYJQkS"
      },
      "outputs": [],
      "source": [
        "# Exercise 2.1: Word-Level Vocabulary and Sequential Dataset\n",
        "\n",
        "# You can reuse the tokenize function from Exercise 0.2 (already defined above)\n",
        "\n",
        "class WordDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A PyTorch Dataset for word-level language modeling.\n",
        "\n",
        "    Creates input-target pairs using a sliding window over tokenized text.\n",
        "    Handles vocabulary building and out-of-vocabulary words.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, text: str, seq_len: int, vocab_size: int = 30000):\n",
        "        \"\"\"\n",
        "        Initialize the dataset.\n",
        "\n",
        "        Args:\n",
        "            text: The full text as a string\n",
        "            seq_len: Length of each sequence (number of words)\n",
        "            vocab_size: Maximum vocabulary size (most frequent words)\n",
        "        \"\"\"\n",
        "        # Task 2.1: START STUDENT CODE\n",
        "\n",
        "        # HINT:\n",
        "        # 1. Store seq_len and tokenize the text\n",
        "        # 2. Build vocabulary: count token frequencies, keep most common up to vocab_size\n",
        "        # 3. Create bidirectional mappings with an <UNK> token for OOV words\n",
        "        # 4. Encode all tokens to indices\n",
        "\n",
        "        # 1. Store sequence length\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        # 2. Tokenize text (reuse tokenizer from Exercise 0.2)\n",
        "        tokens = tokenize(text)\n",
        "\n",
        "        # 3. Count token frequencies\n",
        "        token_counts = Counter(tokens)\n",
        "\n",
        "        # 4. Keep most common tokens (reserve 1 spot for <UNK>)\n",
        "        most_common = token_counts.most_common(vocab_size - 1)\n",
        "\n",
        "        # 5. Build vocabulary with <UNK> token\n",
        "        self.word_to_idx = {\"<UNK>\": 0}\n",
        "        for idx, (word, _) in enumerate(most_common, start=1):\n",
        "            self.word_to_idx[word] = idx\n",
        "\n",
        "        self.idx_to_word = {idx: word for word, idx in self.word_to_idx.items()}\n",
        "        self.vocab_size = len(self.word_to_idx)\n",
        "\n",
        "        # 6. Encode entire corpus to indices (OOV → <UNK>)\n",
        "        self.data = torch.tensor(\n",
        "            [self.word_to_idx.get(token, 0) for token in tokens],\n",
        "            dtype=torch.long\n",
        "        )\n",
        "\n",
        "        # Task 2.1: END STUDENT CODE\n",
        "\n",
        "    def __len__(self):\n",
        "        # Task 2.2: START STUDENT CODE\n",
        "        \"\"\"Number of samples available.\"\"\"\n",
        "\n",
        "        # HINT: Similar to CharDataset, return the number of valid sequences\n",
        "        # Each sample needs seq_len + 1 tokens\n",
        "        return len(self.data) - self.seq_len\n",
        "\n",
        "        # Task 2.2: END STUDENT CODE\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Task 2.3: START STUDENT CODE\n",
        "        \"\"\"Get a single sample (input, target pair).\"\"\"\n",
        "\n",
        "        # HINT: Implement sliding window for next-word prediction (same as character level)\n",
        "         # Sliding window for next-word prediction\n",
        "        input_seq = self.data[idx : idx + self.seq_len]\n",
        "        target_seq = self.data[idx + 1 : idx + self.seq_len + 1]\n",
        "\n",
        "        return input_seq, target_seq\n",
        "\n",
        "\n",
        "        # Task 2.3: END STUDENT CODE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "PN_R_YhHw9Rj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "813a93a4-2ece-4616-c120-9f37c3fa0b70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Exercise 2.1: Word-Level Dataset ---\n",
            "\n",
            "Loading full Shakespeare corpus...\n",
            "Found 1 works\n",
            "Total corpus length: 142,446 characters\n",
            "\n",
            "Batch shapes: Input torch.Size([64, 100]), Target torch.Size([64, 100])\n",
            "\n",
            "Sample input (first 20 words): thou have with me ? mercutio . good king of cats , nothing but one of your nine lives ;...\n",
            "\n",
            "--- Design Notes ---\n",
            "seq_len=100: Longer context window for word-level modeling\n",
            "vocab_size=30000: Accommodates Shakespeare's vocabulary\n",
            "<UNK> handling: Rare words mapped to single UNK token\n"
          ]
        }
      ],
      "source": [
        "# Load the full corpus for word-level modeling\n",
        "print(\"--- Exercise 2.1: Word-Level Dataset ---\")\n",
        "print(\"\\nLoading full Shakespeare corpus...\")\n",
        "\n",
        "all_text = []\n",
        "work_files = sorted([f for f in os.listdir(WORKS_DIR) if f.endswith('.txt')])\n",
        "# use a selective works if training takes too long\n",
        "work_files = [f for f in work_files if \"romeo\" in f]\n",
        "\n",
        "print(f\"Found {len(work_files)} works\")\n",
        "\n",
        "for filename in work_files:\n",
        "    with open(os.path.join(WORKS_DIR, filename), 'r', encoding='utf-8') as f:\n",
        "        all_text.append(f.read())\n",
        "\n",
        "full_corpus = \"\\n\".join(all_text)\n",
        "print(f\"Total corpus length: {len(full_corpus):,} characters\")\n",
        "\n",
        "# Create dataset\n",
        "WORD_SEQ_LEN = 100  # 100 words of context\n",
        "WORD_BATCH_SIZE = 64\n",
        "WORD_VOCAB_SIZE = 30000\n",
        "\n",
        "word_dataset = WordDataset(full_corpus, WORD_SEQ_LEN, vocab_size=WORD_VOCAB_SIZE)\n",
        "word_dataloader = DataLoader(word_dataset, batch_size=WORD_BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Verify\n",
        "inputs, targets = next(iter(word_dataloader))\n",
        "print(f\"\\nBatch shapes: Input {inputs.shape}, Target {targets.shape}\")\n",
        "\n",
        "# Decode sample\n",
        "sample_input = ' '.join([word_dataset.idx_to_word[i.item()] for i in inputs[0][:20]])\n",
        "print(f\"\\nSample input (first 20 words): {sample_input}...\")\n",
        "\n",
        "print(f\"\\n--- Design Notes ---\")\n",
        "print(f\"seq_len={WORD_SEQ_LEN}: Longer context window for word-level modeling\")\n",
        "print(f\"vocab_size={WORD_VOCAB_SIZE}: Accommodates Shakespeare's vocabulary\")\n",
        "print(f\"<UNK> handling: Rare words mapped to single UNK token\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEEDPce4JQkS"
      },
      "source": [
        "---\n",
        "\n",
        "## Exercise 2.2 – Word-Level RNN Language Model in PyTorch\n",
        "\n",
        "### Description\n",
        "\n",
        "You will implement and train a **word-level RNN-based language model** using the dataset from Exercise 2.1. The model will learn to predict the next word given a sequence of preceding words.\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "After completing this exercise, you should be able to:\n",
        "- Define a word-level recurrent neural language model\n",
        "- Use larger embedding dimensions appropriate for word-level modeling\n",
        "- Train with cross-entropy loss on next-word prediction\n",
        "\n",
        "### Tasks\n",
        "\n",
        "1. Define the `WordRNNLM` model class (similar to CharRNNLM but for words). **(1 point)**\n",
        "2. Train for multiple epochs on the full corpus and save model checkpoints.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "AmlCO9-nJQkS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3988a80-86ef-4124-f449-5932b720245e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Exercise 2.2: Word-Level RNN Model ---\n"
          ]
        }
      ],
      "source": [
        "# Exercise 2.2: Word-Level RNN Language Model\n",
        "\n",
        "\n",
        "class WordRNNLM(nn.Module):\n",
        "    \"\"\"\n",
        "    Word-level RNN Language Model.\n",
        "\n",
        "    Similar architecture to CharRNNLM but with:\n",
        "    - Larger embedding dimensions (words need more representation capacity)\n",
        "    - Multiple RNN layers for better modeling\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size: int, emb_dim: int, hidden_size: int, num_layers: int = 3):\n",
        "        \"\"\"\n",
        "        Initialize the model.\n",
        "\n",
        "        Args:\n",
        "            vocab_size: Number of words in vocabulary\n",
        "            emb_dim: Dimension of word embeddings\n",
        "            hidden_size: Number of hidden units in RNN\n",
        "            num_layers: Number of stacked RNN layers\n",
        "        \"\"\"\n",
        "        # Task 2.4: START STUDENT CODE\n",
        "\n",
        "        # HINT: Build a word-level RNN model (similar to CharRNNLM but with num_layers param)\n",
        "        super().__init__()\n",
        "\n",
        "        # 1. Word embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
        "\n",
        "        # 2. RNN layer (stacked)\n",
        "        self.rnn = nn.RNN(\n",
        "            input_size=emb_dim,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # 3. Output projection layer\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "\n",
        "        # Task 2.4: END STUDENT CODE\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch, seq_len)\n",
        "            hidden: Optional initial hidden state\n",
        "\n",
        "        Returns:\n",
        "            logits: Output logits of shape (batch, seq_len, vocab_size)\n",
        "            hidden: Final hidden state\n",
        "        \"\"\"\n",
        "        # Task 2.5: START STUDENT CODE\n",
        "\n",
        "        # HINT: Forward pass through RNN (same as CharRNNLM)\n",
        "        # 1. Embed word indices → dense vectors\n",
        "        emb = self.embedding(x)  # (batch, seq_len, emb_dim)\n",
        "\n",
        "        # 2. Pass through RNN\n",
        "        rnn_out, hidden = self.rnn(emb, hidden)\n",
        "        # rnn_out: (batch, seq_len, hidden_size)\n",
        "\n",
        "        # 3. Project hidden states to vocabulary logits\n",
        "        logits = self.fc(rnn_out)\n",
        "        # logits: (batch, seq_len, vocab_size)\n",
        "\n",
        "        return logits, hidden\n",
        "\n",
        "        # Task 2.5: END STUDENT CODE\n",
        "\n",
        "print(\"--- Exercise 2.2: Word-Level RNN Model ---\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "t_hZTPptJQkS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48c3f3f2-8573-4fc2-81a2-50b91bfdf96e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model created with 4,543,784 parameters\n",
            "Epoch 1/3 | Batch 0/517 (0.0%) | Loss: 8.2531\n",
            "Epoch 1/3 | Batch 200/517 (38.7%) | Loss: 4.8169\n",
            "Epoch 1/3 | Batch 400/517 (77.4%) | Loss: 2.4180\n",
            "Epoch 1/3 Complete | Avg Loss: 3.9716\n",
            "Epoch 2/3 | Batch 0/517 (0.0%) | Loss: 1.1411\n",
            "Epoch 2/3 | Batch 200/517 (38.7%) | Loss: 0.2923\n",
            "Epoch 2/3 | Batch 400/517 (77.4%) | Loss: 0.1836\n",
            "Epoch 2/3 Complete | Avg Loss: 0.3606\n",
            "Epoch 3/3 | Batch 0/517 (0.0%) | Loss: 0.1513\n",
            "Epoch 3/3 | Batch 200/517 (38.7%) | Loss: 0.1418\n",
            "Epoch 3/3 | Batch 400/517 (77.4%) | Loss: 0.1238\n",
            "Epoch 3/3 Complete | Avg Loss: 0.1359\n",
            "\n",
            "Model saved to word_rnn_model.pth\n"
          ]
        }
      ],
      "source": [
        "# Exercise 2.2 (continued): Training Word RNN\n",
        "\n",
        "# Model hyperparameters - larger than char model\n",
        "WORD_EMB_DIM = 300\n",
        "WORD_HIDDEN_SIZE = 512\n",
        "WORD_NUM_LAYERS = 3\n",
        "WORD_EPOCHS = 3  # Fewer epochs due to larger corpus\n",
        "WORD_LR = 0.001\n",
        "\n",
        "# Create model\n",
        "word_model = WordRNNLM(\n",
        "    vocab_size=word_dataset.vocab_size,\n",
        "    emb_dim=WORD_EMB_DIM,\n",
        "    hidden_size=WORD_HIDDEN_SIZE,\n",
        "    num_layers=WORD_NUM_LAYERS\n",
        ").to(device)\n",
        "\n",
        "print(f\"Model created with {sum(p.numel() for p in word_model.parameters()):,} parameters\")\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(word_model.parameters(), lr=WORD_LR)\n",
        "\n",
        "# Training loop\n",
        "total_batches = len(word_dataloader)\n",
        "word_model.train()\n",
        "\n",
        "for epoch in range(WORD_EPOCHS):\n",
        "    total_loss = 0\n",
        "    for i, (inputs, targets) in enumerate(word_dataloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits, _ = word_model(inputs)\n",
        "        loss = criterion(logits.view(-1, word_dataset.vocab_size), targets.view(-1))\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(word_model.parameters(), 5)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Progress logging\n",
        "        if i % 200 == 0:\n",
        "            pct = (i / total_batches) * 100\n",
        "            print(f\"Epoch {epoch+1}/{WORD_EPOCHS} | Batch {i}/{total_batches} ({pct:.1f}%) | Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / len(word_dataloader)\n",
        "    print(f\"Epoch {epoch+1}/{WORD_EPOCHS} Complete | Avg Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Save model\n",
        "word_model_path = \"word_rnn_model.pth\"\n",
        "torch.save({\n",
        "    'model_state_dict': word_model.state_dict(),\n",
        "    'vocab_size': word_dataset.vocab_size,\n",
        "    'emb_dim': WORD_EMB_DIM,\n",
        "    'hidden_size': WORD_HIDDEN_SIZE,\n",
        "    'num_layers': WORD_NUM_LAYERS,\n",
        "    'word_to_idx': word_dataset.word_to_idx,\n",
        "    'idx_to_word': word_dataset.idx_to_word,\n",
        "}, word_model_path)\n",
        "print(f\"\\nModel saved to {word_model_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujK9u-hdJQkS"
      },
      "source": [
        "---\n",
        "\n",
        "## Exercise 2.3 – Theatrical Chat Interface with a Word-Level RNN\n",
        "\n",
        "### Description\n",
        "\n",
        "In this exercise, you will use your trained **word-level RNN language model** to build a simple **theatrical chat interface**. The model will be prompted with a speaker name and dialog, then continue the text in Shakespearean style.\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "After completing this exercise, you should be able to:\n",
        "- Use a word-level language model for **prompt-based generation**\n",
        "- Design a simple **chat-style interface** around a language model\n",
        "- Control text generation via temperature sampling at the word level\n",
        "\n",
        "### Tasks\n",
        "\n",
        "1. Implement a word-level generation function that detokenizes the generated text after generation. **(1 point)**\n",
        "2. Test with different theatrical prompts (ROMEO., JULIET., etc.) and different temperatures. **(0.5 points)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "-I5a2L3YJQkS"
      },
      "outputs": [],
      "source": [
        "# Exercise 2.3: Theatrical Chat Interface\n",
        "\n",
        "def detokenize(tokens: list) -> str:\n",
        "    \"\"\"\n",
        "    Convert a list of tokens back into readable text.\n",
        "    Handles punctuation attachment (no space before punctuation).\n",
        "\n",
        "    Args:\n",
        "        tokens: List of token strings\n",
        "\n",
        "    Returns:\n",
        "        Reconstructed text string\n",
        "    \"\"\"\n",
        "    text = \"\"\n",
        "    for t in tokens:\n",
        "        # Attach punctuation without leading space\n",
        "        if t in [\".\", \",\", \"?\", \"!\", \":\", \";\", \"'\"] or t.startswith(\"'\"):\n",
        "            if text:\n",
        "                text = text.rstrip() + t + \" \"\n",
        "            else:\n",
        "                text += t + \" \"\n",
        "        else:\n",
        "            text += t + \" \"\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def generate_words(model, start_text: str, max_tokens: int, temperature: float,\n",
        "                   word_to_idx: dict, idx_to_word: dict, device='cpu') -> str:\n",
        "    \"\"\"\n",
        "    Generate text at the word level with temperature sampling.\n",
        "\n",
        "    Args:\n",
        "        model: Trained WordRNNLM model\n",
        "        start_text: Initial prompt text\n",
        "        max_tokens: Maximum number of words to generate\n",
        "        temperature: Sampling temperature\n",
        "        word_to_idx: Word to index mapping\n",
        "        idx_to_word: Index to word mapping\n",
        "        device: Device to run on\n",
        "\n",
        "    Returns:\n",
        "        Generated text string (including prompt)\n",
        "    \"\"\"\n",
        "    # Task 2.6: START STUDENT CODE\n",
        "\n",
        "    # HINT: Word-level generation is similar to character-level but:\n",
        "    # 1. Tokenize the prompt using the tokenize() function\n",
        "    # 2. Map tokens to indices (handle UNK)\n",
        "    # 3. Generate words with temperature sampling (like Task 1.9)\n",
        "    # 4. Detokenize the result (reconstruct readable text with proper spacing)\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    # 1. Tokenize prompt\n",
        "    tokens = tokenize(start_text)\n",
        "\n",
        "    # 2. Convert tokens to indices (use <UNK> for unknown words)\n",
        "    input_indices = [\n",
        "        word_to_idx.get(tok, word_to_idx[\"<UNK>\"]) for tok in tokens\n",
        "    ]\n",
        "\n",
        "    input_tensor = torch.tensor(input_indices, dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "    generated_tokens = tokens.copy()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # 3. Run prompt through model to initialize hidden state\n",
        "        _, hidden = model(input_tensor)\n",
        "\n",
        "        # Last word as starting input\n",
        "        current_word = input_tensor[:, -1:]\n",
        "\n",
        "        for _ in range(max_tokens):\n",
        "            logits, hidden = model(current_word, hidden)\n",
        "            logits = logits.squeeze(1)  # (batch=1, vocab_size)\n",
        "\n",
        "            if temperature < 1e-5:\n",
        "                # Greedy decoding\n",
        "                next_idx = torch.argmax(logits, dim=-1)\n",
        "            else:\n",
        "                # Temperature sampling\n",
        "                scaled_logits = logits / temperature\n",
        "                probs = F.softmax(scaled_logits, dim=-1)\n",
        "                next_idx = torch.multinomial(probs, num_samples=1).squeeze()\n",
        "\n",
        "            next_word = idx_to_word[next_idx.item()]\n",
        "            generated_tokens.append(next_word)\n",
        "\n",
        "            # Feed next word back into model\n",
        "            current_word = next_idx.unsqueeze(0).unsqueeze(1)\n",
        "\n",
        "    # 4. Detokenize tokens into readable text\n",
        "    return detokenize(generated_tokens)\n",
        "\n",
        "    # Task 2.6: END STUDENT CODE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "9JG9Eu0qw9Rk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd3b0503-40fd-418d-b87d-4c02e3141af3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Exercise 2.3: Theatrical Chat Interface ---\n",
            "\n",
            "============================================================\n",
            "ROMEO: My heart is heavy with unspoken words.\n",
            "============================================================\n",
            "\n",
            "[Temperature 0.5]\n",
            "romeo. my heart is heavy with unspoken words. romeo is the fairies' midwife, and she comes in shape no bigger than an agate - stone on the fore - finger of an alderman, drawn with a team of little atomies over men's noses as they lie asleep: her waggon - spokes made of long\n",
            "\n",
            "[Temperature 0.8]\n",
            "romeo. my heart is heavy with unspoken words. romeo. ay, those attires are best. but, gentle nurse, i pray thee leave me to myself tonight; for i have need of many orisons to move the heavens to smile upon my state, which, well thou know'st, is cross and full\n",
            "\n",
            "[Temperature 1.2]\n",
            "romeo. my heart is heavy with unspoken words. life is the fairies' midwife, and she agree, within the gracious and after modesty have; bid thee head to fast, lest purgatory, torture, hell itself lies shrift your ladyship, was such a matter than she would must none shortly, gentlewoman,\n",
            "\n",
            "============================================================\n",
            "JULIET: Good even, my lord. Why art thou troubled?\n",
            "============================================================\n",
            "\n",
            "[Temperature 0.5]\n",
            "juliet. good even, my lord. why art thou troubled? benvolio. balthasar. i do protest now you. mercutio. and so black i. romeo. this day's black fate on mo days doth depend; this but begins the woe others must end. re - enter tybalt. benvolio. here comes the furious tybalt\n",
            "\n",
            "[Temperature 0.8]\n",
            "juliet. good even, my lord. why art thou troubled? nurse and there. capulet. come between me, what says tybalt? nurse. death, that thou wilt propagate to find her centre beloved cross'd, the room of romeo? o, so light a foot will ne'er wear out the everlasting flint. a lover\n",
            "\n",
            "[Temperature 1.2]\n",
            "juliet. good even, my lord. why art thou troubled? capulet, tybalt, citizens and servants. _ ] montague. who set this ancient quarrel new abroach? speak to dinner; but light compliment, have made him tremble, compare a grief by my will, may cross the appertaining low, and nothing appear warm\n",
            "\n",
            "============================================================\n",
            "HAMLET: To be, or not to be, that is the question.\n",
            "============================================================\n",
            "\n",
            "[Temperature 0.5]\n",
            "hamlet. to be, or not to be, that is the question. romeo. now comes the thing, be strong and prosperous in this resolve. i'll send a friar with speed to mantua, with my letters to thy lord. juliet. love give me strength, and strength shall help afford. farewell, dear father. [\n",
            "\n",
            "[Temperature 0.8]\n",
            "hamlet. to be, or not to be, that is the question. romeo. o, thou wilt speak again of banishment. friar lawrence. i'll give thee armour to keep off that word, adversity's sweet milk, philosophy, to comfort thee, though thou art banished. romeo. yet banished? hang up philosophy. unless philosophy\n",
            "\n",
            "[Temperature 1.2]\n",
            "hamlet. to be, or not to be, that is the question. stirs. it written. will she leans her cheek at blood of mine marriage in fears, to fire hers, no honesty in men both to engrossing paris. capulet. things have fallen out, sir, so unluckily that we have had no time to move\n"
          ]
        }
      ],
      "source": [
        "# Test theatrical generation\n",
        "print(\"--- Exercise 2.3: Theatrical Chat Interface ---\")\n",
        "\n",
        "examples = [\n",
        "    (\"ROMEO\", \"My heart is heavy with unspoken words.\"),\n",
        "    (\"JULIET\", \"Good even, my lord. Why art thou troubled?\"),\n",
        "    (\"HAMLET\", \"To be, or not to be, that is the question.\")\n",
        "]\n",
        "\n",
        "for speaker, line in examples:\n",
        "    prompt = f\"{speaker}.\\n{line}\\n\"\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"{speaker}: {line}\")\n",
        "    print('='*60)\n",
        "\n",
        "    for temp in [0.5, 0.8, 1.2]:\n",
        "        response = generate_words(\n",
        "            word_model, prompt, 50, temp,\n",
        "            word_dataset.word_to_idx, word_dataset.idx_to_word, device\n",
        "        )\n",
        "        print(f\"\\n[Temperature {temp}]\")\n",
        "        print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7nmjPezJQkS"
      },
      "source": [
        "---\n",
        "\n",
        "## Exercise 2.4 – Turn-Based Modeling with Special Tokens\n",
        "\n",
        "### Description\n",
        "\n",
        "To build a realistic theatrical chat interface, the model needs to understand when a speaker's turn ends. In this exercise, you will create a specialized dataset that inserts a special **End-of-Turn** token (`<EOS>`) before every speaker change.\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "- Preprocess text to explicitly model dialog structure (turns)\n",
        "- Use special tokens (`<EOS>`) to control generation length\n",
        "- Train a language model that learns to stop generating at appropriate times\n",
        "\n",
        "### Key Insight\n",
        "\n",
        "By training with `<EOS>` markers before speaker changes, the model learns:\n",
        "1. When to stop generating (predict `<EOS>`)\n",
        "2. The natural rhythm of theatrical dialogue\n",
        "\n",
        "### Tasks\n",
        "\n",
        "1. Implement a `insert_turn_markers()` to add `<EOS>` before speaker names. **(1 point)**\n",
        "2. Create a `TurnDataset` with the modified corpus and train a turn-aware model. **(1 point)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "2UwEeuDtJQkT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f113f9bd-1815-46d6-c831-031f894f0b2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Exercise 2.4: Turn-Based Dataset ---\n",
            "\n",
            "Turn Dataset Stats:\n",
            "  Vocabulary size: 3785\n",
            "  EOS token index: 1\n",
            "  Number of samples: 33902\n"
          ]
        }
      ],
      "source": [
        "# Exercise 2.4: Turn-Based Modeling with Special Tokens\n",
        "\n",
        "def insert_turn_markers(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Insert <EOS> markers before speaker names in the text.\n",
        "\n",
        "    Speaker detection heuristic:\n",
        "    - Lines that are predominantly uppercase\n",
        "    - Short (< 30 characters)\n",
        "    - Not empty\n",
        "\n",
        "    Args:\n",
        "        text: Original text\n",
        "\n",
        "    Returns:\n",
        "        Text with <EOS> markers inserted before speaker names\n",
        "    \"\"\"\n",
        "    # Task 2.7: START STUDENT CODE\n",
        "\n",
        "    # HINT: Detect speaker names and insert <EOS> markers\n",
        "    # 1. Split text into lines\n",
        "    # 2. For each line, detect if it's a speaker name:\n",
        "    #    - Non-empty, short (< 30 chars), mostly uppercase\n",
        "    # 3. If speaker: insert \"<EOS>\" before the line\n",
        "    # 4. Rejoin lines\n",
        "    lines = text.splitlines()\n",
        "    processed_lines = []\n",
        "\n",
        "    for line in lines:\n",
        "        stripped = line.strip()\n",
        "\n",
        "        # Speaker detection heuristic\n",
        "        if (\n",
        "            stripped\n",
        "            and len(stripped) < 30\n",
        "            and stripped.upper() == stripped\n",
        "            and any(c.isalpha() for c in stripped)\n",
        "        ):\n",
        "            # Insert EOS before speaker name\n",
        "            processed_lines.append(\"<EOS>\")\n",
        "            processed_lines.append(stripped)\n",
        "        else:\n",
        "            processed_lines.append(line)\n",
        "\n",
        "    return \"\\n\".join(processed_lines)\n",
        "\n",
        "\n",
        "    # Task 2.7: END STUDENT CODE\n",
        "\n",
        "\n",
        "class TurnDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset with turn markers for dialogue modeling.\n",
        "\n",
        "    Similar to WordDataset but:\n",
        "    - Preprocesses text with <EOS> markers\n",
        "    - Ensures <EOS> token is in vocabulary\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, text: str, seq_len: int, vocab_size: int = 30000):\n",
        "        # Task 2.8: START STUDENT CODE\n",
        "\n",
        "        # HINT: Similar to WordDataset but with turn markers:\n",
        "        # 1. Store seq_len\n",
        "        # 2. Insert <EOS> markers before speakers (use insert_turn_markers)\n",
        "        # 3. Replace <EOS> with \"eos_marker\" so tokenizer handles it properly\n",
        "        # 4. Tokenize and build vocabulary (reserve space for UNK and eos_marker)\n",
        "        # 5. Ensure both UNK and EOS tokens are in vocabulary\n",
        "        # 6. Encode all tokens\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        # 1. Insert EOS markers\n",
        "        text_with_turns = insert_turn_markers(text)\n",
        "\n",
        "        # 2. Replace <EOS> with a tokenizer-friendly token\n",
        "        self.eos_token = \"eos_marker\"\n",
        "        text_with_turns = text_with_turns.replace(\"<EOS>\", self.eos_token)\n",
        "\n",
        "        # 3. Tokenize text\n",
        "        tokens = tokenize(text_with_turns)\n",
        "\n",
        "        # 4. Count token frequencies\n",
        "        token_counts = Counter(tokens)\n",
        "\n",
        "        # 5. Reserve indices for UNK and EOS\n",
        "        self.word_to_idx = {\n",
        "            \"<UNK>\": 0,\n",
        "            self.eos_token: 1\n",
        "        }\n",
        "\n",
        "        # 6. Build vocabulary (excluding reserved tokens)\n",
        "        most_common = [\n",
        "            (tok, cnt) for tok, cnt in token_counts.most_common()\n",
        "            if tok not in self.word_to_idx\n",
        "        ][: vocab_size - 2]\n",
        "\n",
        "        for idx, (word, _) in enumerate(most_common, start=2):\n",
        "            self.word_to_idx[word] = idx\n",
        "\n",
        "        self.idx_to_word = {idx: word for word, idx in self.word_to_idx.items()}\n",
        "        self.vocab_size = len(self.word_to_idx)\n",
        "\n",
        "        # 7. Encode tokens\n",
        "        self.data = torch.tensor(\n",
        "            [self.word_to_idx.get(tok, self.word_to_idx[\"<UNK>\"]) for tok in tokens],\n",
        "            dtype=torch.long\n",
        "        )\n",
        "\n",
        "        # Task 2.8: END STUDENT CODE\n",
        "\n",
        "    def __len__(self):\n",
        "        # Task 2.9: START STUDENT CODE\n",
        "\n",
        "        # HINT: Same as WordDataset\n",
        "        return len(self.data) - self.seq_len\n",
        "\n",
        "        # Task 2.9: END STUDENT CODE\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Task 2.10: START STUDENT CODE\n",
        "\n",
        "        # HINT: Same as WordDataset\n",
        "        input_seq = self.data[idx: idx + self.seq_len]\n",
        "        target_seq = self.data[idx + 1: idx + self.seq_len + 1]\n",
        "        return input_seq, target_seq\n",
        "\n",
        "        # Task 2.10: END STUDENT CODE\n",
        "\n",
        "# Create turn-aware dataset\n",
        "print(\"--- Exercise 2.4: Turn-Based Dataset ---\")\n",
        "\n",
        "turn_dataset = TurnDataset(full_corpus, WORD_SEQ_LEN, vocab_size=WORD_VOCAB_SIZE)\n",
        "turn_dataloader = DataLoader(turn_dataset, batch_size=WORD_BATCH_SIZE, shuffle=True)\n",
        "\n",
        "print(f\"\\nTurn Dataset Stats:\")\n",
        "print(f\"  Vocabulary size: {turn_dataset.vocab_size}\")\n",
        "print(f\"  EOS token index: {turn_dataset.word_to_idx[turn_dataset.eos_token]}\")\n",
        "print(f\"  Number of samples: {len(turn_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "HNLV0fxgJQkT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28692817-4b23-4f51-81be-c7810bd2a090"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Training Turn-Aware RNN ---\n",
            "Turn model created with 4,544,597 parameters\n",
            "Epoch 1/3 | Batch 0/530 (0.0%) | Loss: 8.2422\n",
            "Epoch 1/3 | Batch 200/530 (37.7%) | Loss: 3.6083\n",
            "Epoch 1/3 | Batch 400/530 (75.5%) | Loss: 0.8541\n",
            "Epoch 1/3 Complete | Avg Loss: 2.7877\n",
            "Epoch 2/3 | Batch 0/530 (0.0%) | Loss: 0.2965\n",
            "Epoch 2/3 | Batch 200/530 (37.7%) | Loss: 0.1781\n",
            "Epoch 2/3 | Batch 400/530 (75.5%) | Loss: 0.1502\n",
            "Epoch 2/3 Complete | Avg Loss: 0.1816\n",
            "Epoch 3/3 | Batch 0/530 (0.0%) | Loss: 0.1219\n",
            "Epoch 3/3 | Batch 200/530 (37.7%) | Loss: 0.1188\n",
            "Epoch 3/3 | Batch 400/530 (75.5%) | Loss: 0.1168\n",
            "Epoch 3/3 Complete | Avg Loss: 0.1183\n",
            "\n",
            "Turn-aware model saved to turn_rnn_model.pth\n"
          ]
        }
      ],
      "source": [
        "# Exercise 2.4 (continued): Training Turn-Aware Model\n",
        "\n",
        "print(\"--- Training Turn-Aware RNN ---\")\n",
        "\n",
        "# Create turn-aware model (same architecture, different vocab/data)\n",
        "turn_model = WordRNNLM(\n",
        "    vocab_size=turn_dataset.vocab_size,\n",
        "    emb_dim=WORD_EMB_DIM,\n",
        "    hidden_size=WORD_HIDDEN_SIZE,\n",
        "    num_layers=WORD_NUM_LAYERS\n",
        ").to(device)\n",
        "\n",
        "print(f\"Turn model created with {sum(p.numel() for p in turn_model.parameters()):,} parameters\")\n",
        "\n",
        "# Training setup\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(turn_model.parameters(), lr=WORD_LR)\n",
        "\n",
        "# Training loop\n",
        "total_batches = len(turn_dataloader)\n",
        "turn_model.train()\n",
        "\n",
        "TURN_EPOCHS = 3\n",
        "\n",
        "for epoch in range(TURN_EPOCHS):\n",
        "    total_loss = 0\n",
        "    for i, (inputs, targets) in enumerate(turn_dataloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits, _ = turn_model(inputs)\n",
        "        loss = criterion(logits.view(-1, turn_dataset.vocab_size), targets.view(-1))\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(turn_model.parameters(), 5)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if i % 200 == 0:\n",
        "            pct = (i / total_batches) * 100\n",
        "            print(f\"Epoch {epoch+1}/{TURN_EPOCHS} | Batch {i}/{total_batches} ({pct:.1f}%) | Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / len(turn_dataloader)\n",
        "    print(f\"Epoch {epoch+1}/{TURN_EPOCHS} Complete | Avg Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Save turn-aware model\n",
        "turn_model_path = \"turn_rnn_model.pth\"\n",
        "torch.save({\n",
        "    'model_state_dict': turn_model.state_dict(),\n",
        "    'vocab_size': turn_dataset.vocab_size,\n",
        "    'emb_dim': WORD_EMB_DIM,\n",
        "    'hidden_size': WORD_HIDDEN_SIZE,\n",
        "    'num_layers': WORD_NUM_LAYERS,\n",
        "    'word_to_idx': turn_dataset.word_to_idx,\n",
        "    'idx_to_word': turn_dataset.idx_to_word,\n",
        "    'eos_token': turn_dataset.eos_token,\n",
        "}, turn_model_path)\n",
        "print(f\"\\nTurn-aware model saved to {turn_model_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Oa6RZ5Cw9Rl"
      },
      "source": [
        "---\n",
        "\n",
        "## Stage 2.5 – Theatrical Chat Interface (Model Comparison)\n",
        "\n",
        "### Description\n",
        "\n",
        "You can now use your trained models to build a chat interface that supports **both** the standard Word-RNN (from Exercise 2.2) and the Turn-Aware RNN (from Exercise 2.4).\n",
        "\n",
        "### Key Differences\n",
        "\n",
        "| Model | Generation Behavior |\n",
        "|-------|---------------------|\n",
        "| Standard Word-RNN | Generates exactly `max_tokens` words |\n",
        "| Turn-Aware RNN | Stops when `<EOS>` is generated |\n",
        "\n",
        "### Tasks\n",
        "\n",
        "No tasks, just execute the code below and play around with it, to get a feeling for the performance of the two variants.\n",
        "\n",
        "**The `generate_with_eos` function will be useful in the next tasks - you can and should use it, and should familiarize yourself with what it does.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "zLX3Q-mdw9Rm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3a697ca-6289-4aa0-fe9a-a84572283edd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Exercise 2.5: Model Comparison ---\n",
            "\n",
            "======================================================================\n",
            "PROMPT: ROMEO: My heart is heavy with unspoken words.\n",
            "======================================================================\n",
            "\n",
            "[Standard Word-RNN (50 tokens)]\n",
            "[ _exeunt. _ ] scene ii. a street. enter benvolio. benvolio. i do find thy centre. lady capulet. well, think of marriage now: younger than you, here in verona, ladies of esteem, are made already mothe...\n",
            "\n",
            "[Turn-Aware RNN (EOS stopping)]\n",
            "juliet. tybalt is his pleasure to speak. enter nurse.\n",
            "\n",
            "======================================================================\n",
            "PROMPT: JULIET: Good even, my lord. Why art thou troubled?\n",
            "======================================================================\n",
            "\n",
            "[Standard Word-RNN (50 tokens)]\n",
            "hast in the prince's summer upon thy life shall signify again. gregory. look to like a dead man in a skilless soldier's flask, is set afire by thine own ignorance, and thou dismember'd with thine own ...\n",
            "\n",
            "[Turn-Aware RNN (EOS stopping)]\n",
            "romeo. and bad'st me bury love.\n",
            "\n",
            "[Turn-Aware LSTM]\n",
            "nurse. father, herself than the capulets; nay in, and proud dead in keep to brine makes him worm, which that poison stain'd with an cheeks is all in his substance, a very power? now to be gone, she is that so old shall when thou wilt tutor me, which is already sweet as seeming i may the forfeit me to beauty's times. romeo.\n"
          ]
        }
      ],
      "source": [
        "# Exercise 2.5: Theatrical Chat Interface (Model Comparison)\n",
        "\n",
        "def generate_with_eos(model, start_text: str, max_tokens: int, temperature: float,\n",
        "                      word_to_idx: dict, idx_to_word: dict, eos_token: str = None,\n",
        "                      device='cpu') -> str:\n",
        "    \"\"\"\n",
        "    Generate text with optional EOS stopping.\n",
        "\n",
        "    Args:\n",
        "        model: Trained language model\n",
        "        start_text: Initial prompt\n",
        "        max_tokens: Maximum tokens to generate\n",
        "        temperature: Sampling temperature\n",
        "        word_to_idx: Word to index mapping\n",
        "        idx_to_word: Index to word mapping\n",
        "        eos_token: If provided, stop when this token is generated\n",
        "        device: Device to run on\n",
        "\n",
        "    Returns:\n",
        "        Generated text (excluding prompt tokens)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    hidden = None\n",
        "\n",
        "    # Preprocess prompt - handle <eos> placeholder\n",
        "    text_to_process = start_text.lower()\n",
        "    if eos_token and \"<eos>\" in text_to_process:\n",
        "        text_to_process = text_to_process.replace(\"<eos>\", eos_token)\n",
        "\n",
        "    tokens = tokenize(text_to_process)\n",
        "    unk_idx = word_to_idx.get(\"<UNK>\", 0)\n",
        "\n",
        "    input_indices = [word_to_idx.get(t, unk_idx) for t in tokens]\n",
        "    input_seq = torch.tensor(input_indices, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    generated_tokens = []\n",
        "\n",
        "    # Get EOS index if applicable\n",
        "    eos_idx = word_to_idx.get(eos_token, -1) if eos_token else -1\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits, hidden = model(input_seq, hidden)\n",
        "        last_logits = logits[:, -1, :]\n",
        "\n",
        "        for _ in range(max_tokens):\n",
        "            if temperature <= 0:\n",
        "                idx = torch.argmax(last_logits, dim=-1).item()\n",
        "            else:\n",
        "                probs = F.softmax(last_logits / temperature, dim=-1)\n",
        "                idx = torch.multinomial(probs, 1).item()\n",
        "\n",
        "            # Check for EOS\n",
        "            if eos_token and idx == eos_idx:\n",
        "                break\n",
        "\n",
        "            word = idx_to_word.get(idx, \"<UNK>\")\n",
        "            generated_tokens.append(word)\n",
        "\n",
        "            input_seq = torch.tensor([[idx]], dtype=torch.long).to(device)\n",
        "            logits, hidden = model(input_seq, hidden)\n",
        "            last_logits = logits[:, -1, :]\n",
        "\n",
        "    return detokenize(generated_tokens)\n",
        "\n",
        "# Compare models\n",
        "print(\"--- Exercise 2.5: Model Comparison ---\")\n",
        "\n",
        "examples = [\n",
        "    (\"ROMEO\", \"My heart is heavy with unspoken words.\"),\n",
        "    (\"JULIET\", \"Good even, my lord. Why art thou troubled?\"),\n",
        "]\n",
        "\n",
        "for speaker, line in examples:\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"PROMPT: {speaker}: {line}\")\n",
        "    print('='*70)\n",
        "\n",
        "    # Standard model (fixed length)\n",
        "    standard_prompt = f\"{speaker}.\\n{line}\\n\"\n",
        "    standard_response = generate_with_eos(\n",
        "        word_model, standard_prompt, 50, 0.8,\n",
        "        word_dataset.word_to_idx, word_dataset.idx_to_word,\n",
        "        eos_token=None, device=device\n",
        "    )\n",
        "    print(f\"\\n[Standard Word-RNN (50 tokens)]\")\n",
        "    print(standard_response[:200] + \"...\" if len(standard_response) > 200 else standard_response)\n",
        "\n",
        "    # Turn-aware model (EOS stopping)\n",
        "    turn_prompt = f\"<eos>\\n{speaker}.\\n{line}\\n<eos>\\n\"\n",
        "    turn_response = generate_with_eos(\n",
        "        turn_model, turn_prompt, 200, 0.8,\n",
        "        turn_dataset.word_to_idx, turn_dataset.idx_to_word,\n",
        "        eos_token=turn_dataset.eos_token, device=device\n",
        "    )\n",
        "    print(f\"\\n[Turn-Aware RNN (EOS stopping)]\")\n",
        "    print(turn_response)\n",
        "\n",
        "    # LSTM model\n",
        "    turn_lstm_response = generate_with_eos(\n",
        "    lstm_model,                     #  LSTM plugged here\n",
        "    turn_prompt, 200, 0.8,\n",
        "    turn_dataset.word_to_idx, turn_dataset.idx_to_word,\n",
        "    eos_token=turn_dataset.eos_token, device=device\n",
        ")\n",
        "print(f\"\\n[Turn-Aware LSTM]\")\n",
        "print(turn_lstm_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#what is observed here is only LSTM model preserve the long text until #the end. on the other hand, the RNN model stucked after while."
      ],
      "metadata": {
        "id": "gVu-KY0-9Ht0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0ih4uoNJQkT"
      },
      "source": [
        "---\n",
        "\n",
        "# Stage 3: Word-Level LSTM & RNN–LSTM Comparison\n",
        "\n",
        "In this final stage, you will extend your word-level language model by replacing the RNN with an **LSTM**. You will:\n",
        "\n",
        "- Implement and train a **word-level LSTM language model**\n",
        "- Plug the LSTM into your existing **theatrical chat interface**\n",
        "- Qualitatively compare the behavior of **RNN** vs **LSTM** models\n",
        "\n",
        "## Why LSTM?\n",
        "\n",
        "Long Short-Term Memory (LSTM) networks address the **vanishing gradient problem** in standard RNNs:\n",
        "\n",
        "| Feature | RNN | LSTM |\n",
        "|---------|-----|------|\n",
        "| Memory | Short-term only | Long and short-term |\n",
        "| Gradient flow | Degrades over long sequences | Gates preserve gradients |\n",
        "| Training | Faster per step | More stable |\n",
        "| Parameters | Fewer | ~4x more (3 gates + cell) |\n",
        "\n",
        "---\n",
        "\n",
        "## Exercise 3.1 – Word-Level LSTM Language Model in PyTorch\n",
        "\n",
        "### Description\n",
        "\n",
        "You will modify your word-level language model by replacing the RNN layer with an LSTM.\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "- Implement a **word-level LSTM-based language model**\n",
        "- Handle LSTM's dual hidden state `(h, c)`\n",
        "- Train on the same turn-aware dataset for fair comparison\n",
        "\n",
        "### Tasks\n",
        "\n",
        "1. Define `WordLSTMLM` class with `nn.LSTM`. It has to fulfill the same conditions as normally, except this time, the model should return both the prediction and the LSTM's hidden state. **(1 point)**\n",
        "2. Train on the same dataset as the RNN and compare training behavior in terms of loss, stability, etc. **(1 point)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "p3KWm_YCJQkT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e680f44a-ca37-4162-ca4d-50356f87fb83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Exercise 3.1: Word-Level LSTM Model ---\n"
          ]
        }
      ],
      "source": [
        "# Exercise 3.1: Word-Level LSTM Language Model\n",
        "\n",
        "class WordLSTMLM(nn.Module):\n",
        "    \"\"\"\n",
        "    Word-level LSTM Language Model.\n",
        "\n",
        "    Key difference from RNN:\n",
        "    - Uses nn.LSTM instead of nn.RNN\n",
        "    - Hidden state is a tuple (h_n, c_n) where:\n",
        "      - h_n: hidden state (same as RNN)\n",
        "      - c_n: cell state (LSTM's long-term memory)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size: int, emb_dim: int, hidden_size: int, num_layers: int = 3):\n",
        "        \"\"\"\n",
        "        Initialize the LSTM model.\n",
        "\n",
        "        Args:\n",
        "            vocab_size: Number of words in vocabulary\n",
        "            emb_dim: Dimension of word embeddings\n",
        "            hidden_size: Number of hidden units in LSTM\n",
        "            num_layers: Number of stacked LSTM layers\n",
        "        \"\"\"\n",
        "        # Task 3.1: START STUDENT CODE\n",
        "\n",
        "        # HINT: Build a word-level LSTM model (same as WordRNNLM but use nn.LSTM)\n",
        "        super().__init__()\n",
        "\n",
        "        # 1. Word embedding layer\n",
        "        #    Converts word indices -> dense vectors\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
        "\n",
        "        # 2. LSTM layers\n",
        "        #    batch_first=True → input shape (batch, seq_len, emb_dim)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=emb_dim,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # 3. Output projection layer\n",
        "        #    Maps hidden states -> vocabulary logits\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "\n",
        "        # Task 3.1: END STUDENT CODE\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch, seq_len)\n",
        "            hidden: Optional initial hidden state tuple (h_0, c_0)\n",
        "\n",
        "        Returns:\n",
        "            logits: Output logits of shape (batch, seq_len, vocab_size)\n",
        "            hidden: Final hidden state tuple (h_n, c_n)\n",
        "        \"\"\"\n",
        "        # Task 3.2: START STUDENT CODE\n",
        "\n",
        "        # HINT: Forward pass through LSTM (same as RNN forward pass)\n",
        "         # 1. Embed word indices\n",
        "        #    (batch, seq_len) -> (batch, seq_len, emb_dim)\n",
        "        emb = self.embedding(x)\n",
        "\n",
        "        # 2. Pass through LSTM\n",
        "        #    If hidden is None, PyTorch initializes zeros internally\n",
        "        outputs, hidden = self.lstm(emb, hidden)\n",
        "\n",
        "        # 3. Project LSTM outputs to vocabulary space\n",
        "        #    (batch, seq_len, hidden_size) -> (batch, seq_len, vocab_size)\n",
        "        logits = self.fc(outputs)\n",
        "\n",
        "        return logits, hidden\n",
        "\n",
        "        # Task 3.2: END STUDENT CODE\n",
        "\n",
        "\n",
        "print(\"--- Exercise 3.1: Word-Level LSTM Model ---\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "nKo0P-WXJQkT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a803b40-b833-4f51-8413-55026cabbbdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM model created with 8,946,773 parameters\n",
            "(Compare to RNN: ~4,544,597 parameters)\n",
            "Epoch 1/3 | Batch 0/530 (0.0%) | Loss: 8.2455\n",
            "Epoch 1/3 | Batch 200/530 (37.7%) | Loss: 5.8294\n",
            "Epoch 1/3 | Batch 400/530 (75.5%) | Loss: 4.9795\n",
            "Epoch 1/3 Complete | Avg Loss: 5.4508\n",
            "Epoch 2/3 | Batch 0/530 (0.0%) | Loss: 4.6275\n",
            "Epoch 2/3 | Batch 200/530 (37.7%) | Loss: 4.0234\n",
            "Epoch 2/3 | Batch 400/530 (75.5%) | Loss: 3.4968\n",
            "Epoch 2/3 Complete | Avg Loss: 3.8498\n",
            "Epoch 3/3 | Batch 0/530 (0.0%) | Loss: 3.1676\n",
            "Epoch 3/3 | Batch 200/530 (37.7%) | Loss: 2.7167\n",
            "Epoch 3/3 | Batch 400/530 (75.5%) | Loss: 2.3432\n",
            "Epoch 3/3 Complete | Avg Loss: 2.5890\n",
            "\n",
            "LSTM model saved to word_lstm_model.pth\n"
          ]
        }
      ],
      "source": [
        "# Exercise 3.1 (continued): Training LSTM Model\n",
        "\n",
        "# Create LSTM model (same hyperparameters as RNN for fair comparison)\n",
        "lstm_model = WordLSTMLM(\n",
        "    vocab_size=turn_dataset.vocab_size,\n",
        "    emb_dim=WORD_EMB_DIM,\n",
        "    hidden_size=WORD_HIDDEN_SIZE,\n",
        "    num_layers=WORD_NUM_LAYERS\n",
        ").to(device)\n",
        "\n",
        "print(f\"LSTM model created with {sum(p.numel() for p in lstm_model.parameters()):,} parameters\")\n",
        "print(f\"(Compare to RNN: ~{sum(p.numel() for p in turn_model.parameters()):,} parameters)\")\n",
        "\n",
        "# Training setup\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(lstm_model.parameters(), lr=WORD_LR)\n",
        "\n",
        "# Training loop\n",
        "total_batches = len(turn_dataloader)\n",
        "lstm_model.train()\n",
        "\n",
        "LSTM_EPOCHS = 3\n",
        "\n",
        "for epoch in range(LSTM_EPOCHS):\n",
        "    total_loss = 0\n",
        "    for i, (inputs, targets) in enumerate(turn_dataloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits, _ = lstm_model(inputs)\n",
        "        loss = criterion(logits.view(-1, turn_dataset.vocab_size), targets.view(-1))\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(lstm_model.parameters(), 5)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if i % 200 == 0:\n",
        "            pct = (i / total_batches) * 100\n",
        "            print(f\"Epoch {epoch+1}/{LSTM_EPOCHS} | Batch {i}/{total_batches} ({pct:.1f}%) | Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / len(turn_dataloader)\n",
        "    print(f\"Epoch {epoch+1}/{LSTM_EPOCHS} Complete | Avg Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Save LSTM model\n",
        "lstm_model_path = \"word_lstm_model.pth\"\n",
        "torch.save({\n",
        "    'model_state_dict': lstm_model.state_dict(),\n",
        "    'vocab_size': turn_dataset.vocab_size,\n",
        "    'emb_dim': WORD_EMB_DIM,\n",
        "    'hidden_size': WORD_HIDDEN_SIZE,\n",
        "    'num_layers': WORD_NUM_LAYERS,\n",
        "    'word_to_idx': turn_dataset.word_to_idx,\n",
        "    'idx_to_word': turn_dataset.idx_to_word,\n",
        "    'eos_token': turn_dataset.eos_token,\n",
        "}, lstm_model_path)\n",
        "print(f\"\\nLSTM model saved to {lstm_model_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RNN show a smaller average loss after 3 epochs which mean it converg faster in cotrty to LSTM which show\n",
        "# a greater average loss after 3 epoches which stands for it has to learn more parameters. because of that it is slower.\n",
        "# LSTM need more epoches to learn better."
      ],
      "metadata": {
        "id": "b2a-2BHDOJ0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MXouAiAJQkT"
      },
      "source": [
        "---\n",
        "\n",
        "## Exercise 3.2 – LSTM Chat Interface and RNN–LSTM Comparison\n",
        "\n",
        "### Description\n",
        "\n",
        "You will now plug your **word-level LSTM language model** into the theatrical chat interface and compare its behavior to the **word-level RNN** using identical prompts and settings.\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "- Use a word-level LSTM for **prompt-based generation**\n",
        "- Compare outputs of RNN vs LSTM models\n",
        "- Reflect on advantages and limitations of both architectures\n",
        "\n",
        "### Tasks\n",
        "\n",
        "Plug the LSTM model into the chat interface you made and compare to the RNN chat generation, using the following comparison criteria **(1 point)**:\n",
        "\n",
        "1. **Coherence**: Does the text make grammatical sense?\n",
        "2. **Dialog structure**: Does it follow speaker patterns?\n",
        "3. **Repetition**: Does one model repeat itself more?\n",
        "4. **Creativity**: Which produces more varied outputs?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ZVlFecyyJQkT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d52a9bb7-3096-41ca-e29d-ca16ff20d90f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Exercise 3.2: RNN vs LSTM Comparison ---\n",
            "\n",
            "======================================================================\n",
            "ROMEO: My heart is heavy with unspoken words.\n",
            "(Response from: JULIET)\n",
            "======================================================================\n",
            "\n",
            "[RNN] JULIET:\n",
            "one of the doors, and say care nay better.\n",
            "\n",
            "[LSTM] JULIET:\n",
            "o good heart's flowers. a open quarrelled, and pass now? what oddly rosaline.\n",
            "\n",
            "======================================================================\n",
            "JULIET: Good even, my lord. Why art thou troubled?\n",
            "(Response from: ROMEO)\n",
            "======================================================================\n",
            "\n",
            "[RNN] ROMEO:\n",
            "and bad'st me bury love.\n",
            "\n",
            "[LSTM] ROMEO:\n",
            "mercutio's master's?\n",
            "\n",
            "======================================================================\n",
            "HAMLET: To be, or not to be, I ask again.\n",
            "(Response from: HORATIO)\n",
            "======================================================================\n",
            "\n",
            "[RNN] HORATIO:\n",
            "thou wast never with me for anything, when thou wast not there for the goose.\n",
            "\n",
            "[LSTM] HORATIO:\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Exercise 3.2: RNN vs LSTM Comparison\n",
        "\n",
        "print(\"--- Exercise 3.2: RNN vs LSTM Comparison ---\")\n",
        "\n",
        "# Test prompts\n",
        "comparison_prompts = [\n",
        "    (\"ROMEO\", \"My heart is heavy with unspoken words.\", \"JULIET\"),\n",
        "    (\"JULIET\", \"Good even, my lord. Why art thou troubled?\", \"ROMEO\"),\n",
        "    (\"HAMLET\", \"To be, or not to be, I ask again.\", \"HORATIO\"),\n",
        "]\n",
        "\n",
        "for user_speaker, user_line, model_speaker in comparison_prompts:\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"{user_speaker}: {user_line}\")\n",
        "    print(f\"(Response from: {model_speaker})\")\n",
        "    print('='*70)\n",
        "\n",
        "    # Construct prompts\n",
        "    prompt = f\"<eos>\\n{user_speaker}.\\n{user_line}\\n<eos>\\n{model_speaker}.\\n\"\n",
        "\n",
        "    # RNN response\n",
        "    # Task 3.3: START STUDENT CODE\n",
        "\n",
        "    # HINT: Generate rnn_response using the turn_model (RNN) with the prompt\n",
        "    rnn_response = generate_with_eos(\n",
        "        model=turn_model,\n",
        "        start_text=prompt,\n",
        "        max_tokens=200,\n",
        "        temperature=0.8,\n",
        "        word_to_idx=turn_dataset.word_to_idx,\n",
        "        idx_to_word=turn_dataset.idx_to_word,\n",
        "        eos_token=turn_dataset.eos_token,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # Task 3.3: END STUDENT CODE\n",
        "\n",
        "    print(f\"\\n[RNN] {model_speaker}:\")\n",
        "    print(rnn_response)\n",
        "\n",
        "    # LSTM response\n",
        "    # Task 3.4: START STUDENT CODE\n",
        "\n",
        "    # HINT: Generate lstm_response using the lstm_model (LSTM) with the same prompt\n",
        "    lstm_response = generate_with_eos(\n",
        "        model=lstm_model,\n",
        "        start_text=prompt,\n",
        "        max_tokens=200,\n",
        "        temperature=0.8,\n",
        "        word_to_idx=turn_dataset.word_to_idx,\n",
        "        idx_to_word=turn_dataset.idx_to_word,\n",
        "        eos_token=turn_dataset.eos_token,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # Task 3.4: END STUDENT CODE\n",
        "    print(f\"\\n[LSTM] {model_speaker}:\")\n",
        "    print(lstm_response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RNN vs LSTM Comparison:\n",
        "In terms of coherence, the RNN produces more grammatically stable but simpler responses, while the LSTM occasionally generates less precise grammar. For dialog structure, the LSTM better reflects theatrical speaker patterns and character interaction. Regarding repetition, the RNN tends to repeat common phrases, whereas the LSTM shows less repetition. In terms of creativity, the LSTM generates more varied and imaginative responses compared to the more conservative RNN outputs."
      ],
      "metadata": {
        "id": "JFa0Rk3YKLwy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9Ydx96iJQkT"
      },
      "source": [
        "---\n",
        "\n",
        "# 🎭 Congratulations!\n",
        "\n",
        "You have successfully completed all exercises in this NLP workshop. Here's what you've accomplished:\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "1. **Data Preprocessing Matters**: Tokenization, normalization, and special tokens significantly impact model behavior.\n",
        "\n",
        "2. **Architecture Choices**: RNNs are simple but struggle with long sequences; LSTMs address this with gating mechanisms.\n",
        "\n",
        "3. **Temperature Sampling**: Controls the creativity-coherence tradeoff in generation.\n",
        "\n",
        "4. **Turn-Aware Training**: Special tokens help models learn dialogue structure.\n",
        "\n",
        "## Further Exploration\n",
        "\n",
        "- Try different hyperparameters (hidden size, layers, learning rate)\n",
        "- Try different combinations of shakespeare's works for training\n",
        "- Try making LSTMs work for turn-based conversations"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}